'''
2020/4/22ã®è¿‘æ³å ±å‘Šä¼šã‚’å—ã‘ã¦
ã¾ãšã¯ARãƒ¢ãƒ‡ãƒ«ã§ä½œæˆã—ãŸäººå·¥æ™‚ç³»åˆ—ã‚’ç”¨ã„ã¦ã€ãã¡ã‚“ã¨ARãƒ¢ãƒ‡ãƒ«ã®ä¿‚æ•°ãŒæ¨å®šã§ãã¦ã„ã‚‹ã®ã‹ã‚’ç¢ºèªã™ã¹ãã€‚

ğºğœƒ ã¨ ğ¹ğœ™ ã¯æ´»æ€§åŒ–é–¢æ•°ã‚’ç›´ç·šã«ã—ãŸä¸€å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ï¼
'''

import argparse
import os
path = os.getcwd()
path=path[:path.find('timeseries-WGAN')+15]
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import math
import sys
sys.path.append(path+"/")
import random
import statsmodels.api as sm
from scipy.stats import norm

import torchvision.transforms as transforms
from torchvision.utils import save_image

import torch.nn as nn
import torch.nn.functional as F
import torch
from torch.autograd import Variable

# äººå·¥ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹æ©Ÿæ¢°ãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import tsModel
# å­¦ç¿’ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import models



# "output-images"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«ã‚ã‚‹ãªã‚‰ãã‚Œã§è‰¯ã—ï¼‰
os.makedirs("output-images", exist_ok=True)
os.makedirs("parameters", exist_ok=True)



# å­¦ç¿’æ™‚ã®ãƒã‚¤ãƒ‘ãƒ©ã®æ±ºå®šï¼ˆå…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ï¼‰
parser = argparse.ArgumentParser()
parser.add_argument("--n_epochs", type=int, default=20000, help="Discriminatorã‚’å­¦ç¿’ã•ã›ã‚‹å›æ•°")
parser.add_argument("--p", type=int, default=7, help="ARã®æ¬¡æ•°(generatorã¸ã®å…¥åŠ›ã®æ¬¡å…ƒ)")
parser.add_argument("--generator_seed", type=int, default=0, help="generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--discriminator_seed", type=int, default=0, help="discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--random_seed", type=int, default=0, help="è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ã®ã©ã®æ™‚åˆ»ã‚’å­¦ç¿’ã«ç”¨ã„ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ±ºå®šã™ã‚‹æ™‚ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--batch_size", type=int, default=64, help="batchã®å¤§ãã•")
parser.add_argument("--discriminator_hidden_unit", type=int, default=64, help="discriminatorã®éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•°")
parser.add_argument("--lr", type=float, default=0.00005, help="å­¦ç¿’ç‡")
parser.add_argument("--n_critic", type=int, default=5, help="ä¸€åº¦generatorã‚’backpropã™ã‚‹ã”ã¨ã«ä½•å›discriminatorã‚’backpropã™ã‚‹ã‹")
parser.add_argument("--sample_interval", type=int, default=1000, help="batchã‚’ä½•å›å­¦ç¿’ã•ã›ã‚‹åº¦ã«generatorã®å‡ºåŠ›ã‚’ä¿å­˜ã™ã‚‹ã‹")
parser.add_argument("--withGP", type=bool, default=False, help="clipingã®ä»£ã‚ã‚Šã«Gradient Penaltyã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚True/False")
parser.add_argument("--withCorr", type=bool, default=False, help="Generatorã®å‡ºåŠ›ãŒbatchæ–¹å‘ã«ç„¡ç›¸é–¢ã«ãªã‚‹ã‚ˆã†ãªãƒ­ã‚¹ã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚ã€€True/False")
parser.add_argument("--data_seed", type=int, default=0, help="Dataã®ä½œæˆã«ç”¨ã„ã‚‹ä¹±æ•°ã®seed")
opt = parser.parse_args()
# opt = parser.parse_args(args=[])

print(opt)

if not opt.withGP:
    clip_value = input('discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’clipã™ã‚‹å€¤(æ­£ã®æ•°)ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„(defaultã¯0.01)ï¼š')
    try:
        clip_value=float(clip_value)
        if clip_value<=0:
            clip_value=0.01
    except:
        clip_value=0.01
    print("clipå€¤ã¯",clip_value,"ã§ã™ã€‚")


# ç›¸é–¢ä¿‚æ•°ã®åˆ¶ç´„ã‚’Lossã«åŠ ãˆã‚‹éš›ã®é‡ã¿ã®è¨­å®š
default_weight = 1
if opt.withCorr:
    corr_weight= input("ç›¸é–¢ä¿‚æ•°ã®åˆ¶ç´„ã‚’Lossã«åŠ ãˆã‚‹æ™‚ã®é‡ã¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(defaultã¯{0})ï¼š".format(default_weight))
    try:
        corr_weight = float(corr_weight)
        if corr_weight<0:
            corr_weight = default_weight
    except:
        corr_weight=default_weight
    print("ç›¸é–¢ä¿‚æ•°ã«é–¢ã™ã‚‹åˆ¶ç´„ã®Losså†…ã§ã®é‡ã¿ã¯{0}ã§ã™ã€‚".format(corr_weight))
        
else:
    corr_weight=0


# gpuãŒä½¿ãˆã‚‹ã‹ã©ã†ã‹
cuda = True if torch.cuda.is_available() else False
if cuda:
    print("GPUãŒä½¿ãˆã¾ã™ã€‚")
    use_gpu = input('GPUã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> ')
    cuda = bool(int(use_gpu))
else:
    print("GPUã¯ä½¿ãˆã¾ã›ã‚“ã€‚")
    
if cuda:
    gpu_id = input('ä½¿ç”¨ã™ã‚‹GPUã®ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ : ')
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
device = torch.device('cuda:'+gpu_id if cuda else 'cpu')





torch.manual_seed(opt.generator_seed)
generator = models.LinearGenerator(p = opt.p, input_dim=1, is_bias=False)

torch.manual_seed(opt.discriminator_seed)
discriminator = models.Discriminator(q=0, discriminator_hidden_unit=opt.discriminator_hidden_unit)



# äººå·¥ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
Data = tsModel.SARIMA(a=[0.3,-0.4,0.3,-0.4,0.3,-0.4,0.3], N=1400, random_seed=opt.data_seed, sigma=2)
Data = torch.tensor(Data, dtype=torch.float)
plt.figure(figsize=(13,8))
plt.plot(Data)
plt.savefig(path+"/images/AR7ãƒ¢ãƒ‡ãƒ«ã®äººå·¥ãƒ‡ãƒ¼ã‚¿ãã®{0}.png".format(opt.data_seed))
plt.close()

Data=Data.view(1,-1)

trainData = Data[:,:1000]
valData = Data[:,1000:1200]
testData = Data[:,1200:]

trainMatrix = []
for i in range(trainData.shape[1]-(opt.p+1)):
    ans = trainData[:,i:i+opt.p+1].view(1,Data.shape[0],-1)
    trainMatrix.append(ans)
trainMatrix = torch.cat(trainMatrix)

valMatrix = []
for i in range(valData.shape[1]-(opt.p+1)):
    ans = valData[:,i:i+opt.p+1].view(1,Data.shape[0],-1)
    valMatrix.append(ans)
valMatrix = torch.cat(valMatrix)



# å­¦ç¿’

# Optimizers(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹)
optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)
optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ä¹—ã£ã‘ã‚‹
generator.to(device)
discriminator.to(device)

trainMatrix=trainMatrix.to(device)
valMatrix=valMatrix.to(device)

def gradient_penalty(generated_data, real_data, gp_weight=10):

    batch_size = real_data.size()[0]

    alpha = torch.rand(batch_size, 1)
    alpha = alpha.expand_as(real_data)
    if cuda:
        alpha=alpha.to(device)

    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data
    interpolated = Variable(interpolated, requires_grad=True)
    if cuda:
        interpolated=interpolated.to(device)

    # Calculate probability of interpolated examples
    prob_interpolated = discriminator(interpolated)

    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,
                               grad_outputs=torch.ones(prob_interpolated.size()).to(device) if cuda else torch.ones(
                               prob_interpolated.size()),
                               create_graph=True, retain_graph=True)[0]

    gradients = gradients.view(batch_size, -1)# ã“ã‚Œã„ã‚‰ãªã„ã‹ã‚‚...
    
    # gradients_norm = (gradients.norm(2, dim=1) - 1) ** 2
    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)# 0é™¤ç®—ã‚’é˜²ãï¼Ÿ

    return gp_weight * ((gradients_norm - 1) ** 2).mean()

def corr(x):
    return (((x-x.mean())*(x-x.mean()).T)*(1-torch.eye(x.shape[0],x.shape[0]).to(device))).sum()/2/x.shape[0]

saveModel = input('ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> ')
saveModel = bool(int(saveModel))

import japanize_matplotlib
from scipy.stats import gaussian_kde

batches_done = 0
generator_done = 0# generatorã‚’å­¦ç¿’ã—ãŸå›æ•°

# ã‚°ãƒ©ãƒ•æç”»ç”¨
loss_D_curve = []
loss_G_curve = []
val_loss_D_curve = []
val_loss_G_curve = []
p_value = []

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ã®ã©ã®æ™‚åˆ»ã‚’å­¦ç¿’ã«ç”¨ã„ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ã„ã‚‹ãŒã€ãã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã™ã‚‹
random.seed(a=opt.random_seed)

for epoch in range(1, opt.n_epochs+1):# epochã”ã¨ã®å‡¦ç†
    
    # trainMatrixã®è¡Œã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹
    # r=torch.randperm(trainMatrix.shape[0])
    # c=torch.arange(trainMatrix.shape[1])
    # trainMatrix = trainMatrix[r[:, None],c]
    
    for i, batch in enumerate(range(0, trainMatrix.shape[0]-opt.batch_size, opt.batch_size)):# batchã”ã¨ã®å‡¦ç†
        
        # generatorã¸ã®å…¥åŠ›ã‚’ç”¨æ„ã™ã‚‹
        X = trainMatrix[batch:batch+opt.batch_size]# torch.Size([64, 1, 8])
        # æ™‚ç³»åˆ—ã®é †ç•ªã¯ãã®ã¾ã¾å…¥åŠ›ã—ãŸæ–¹ãŒã„ã„ã®ã‹ãªï¼Ÿ
        rand=random.randint(0,trainMatrix.shape[0] - trainMatrix.shape[0]// opt.batch_size*opt.batch_size)
        X = trainMatrix[batch+rand : batch+rand+opt.batch_size]# torch.Size([64, 1, 8])
    
        X = Variable(X)# è‡ªå‹•å¾®åˆ†ï¼Ÿ
        
        # ---------------------
        #  Train Discriminator
        # ---------------------
        
        for p in discriminator.parameters(): # reset requires_grad
            p.requires_grad = True # they are set to False below in netG update

        if not opt.withGP:
            # discriminatorã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ï¼ˆå…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã®çµ¶å¯¾å€¤ãŒclip_valueä»¥ä¸‹ã®å€¤ã«ãªã‚‹ï¼‰
            for idx, p in enumerate(discriminator.parameters()):
                if idx==0:
                    continue #  sigmaã¯ã‚¯ãƒªãƒƒãƒ—ã—ãªã„
                p.data.clamp_(-clip_value, clip_value)

        
        # å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–ã™ã‚‹
        optimizer_D.zero_grad()        
        
        # ç¾åœ¨ï¼‹éå»pæ™‚åˆ»åˆ†ã®æ™‚ç³»åˆ—ã‚’generatorã§å¤‰æ›ã—ãŸå€¤ã‚’å–å¾—
        hat_epsilon = generator(X)#.detach() # torch.Size([64, 4])
        # ã“ã®ã€Œ.detach()ã€ã¯Tensorå‹ã‹ã‚‰å‹¾é…æƒ…å ±ã‚’æŠœã„ãŸã‚‚ã®ã‚’å–å¾—ã™ã‚‹.(ã¤ã¾ã‚Šã“ã®å¾Œã®èª¤å·®é€†ä¼æ’­ã®ã¨ã“ã‚ã§ã¯generatorã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã¾ã§ä¼æ’­ã—ãªã„)
        
        # generatorã®å‡ºåŠ›ã¨åŒã˜å¤§ãã•ã®æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
        epsillon = Variable(torch.randn_like(hat_epsilon))
        
        # Adversarial loss ã™ãªã‚ã¡Wassersteinè·é›¢ã®ç¬¦å·ã‚’åè»¢ã•ã›ãŸã‚‚ã®ã€‚ï¼ˆDiscriminatorã¯Wassersteinè·é›¢ã‚’æœ€å¤§ã«ã™ã‚‹é–¢æ•°ã«ãªã‚ŠãŸã„ï¼‰
        if opt.withGP:
            loss_D = -torch.mean(discriminator(epsillon, is_from_generator=False)) + torch.mean(discriminator(hat_epsilon, is_from_generator=True)) + gradient_penalty(generated_data=hat_epsilon, real_data=epsillon, gp_weight=10)
        else:
            loss_D = -torch.mean(discriminator(epsillon, is_from_generator=False)) + torch.mean(discriminator(hat_epsilon, is_from_generator=True))
 

        # loss_Dã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³
        loss_D.backward()
        # otimizerã«ã—ãŸãŒã£ã¦ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’æ›´æ–°
        optimizer_D.step()

            
        # discriminatorã‚’opt.n_criticå›å­¦ç¿’ã•ã›ã‚‹ã”ã¨ã«ä¸€å›generatorã‚’å­¦ç¿’ã•ã›ã‚‹(ãŸã ã—æœ€åˆã¯ã‚ã£ã¡ã‚ƒdiscriminatorã‚’å„ªå…ˆã•ã›ã‚‹)
        if batches_done % (100 if generator_done<25 or generator_done%500==0 else opt.n_critic) == 0:
            
            # -----------------
            #  Train Generator
            # -----------------
            
            for p in discriminator.parameters():
                p.requires_grad = False # to avoid computation
            
            # generatorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–
            optimizer_G.zero_grad()
            
            # ç¾åœ¨ï¼‹éå»pæ™‚åˆ»åˆ†ã®æ™‚ç³»åˆ—ã‚’generatorã§å¤‰æ›ã—ãŸå€¤ã‚’å–å¾—
            hat_epsilon = generator(X)# torch.Size([64, 4]) (ä»Šåº¦ã¯detachã—ãªã„)
            
            # Adversarial loss(discriminatorã®å‡ºåŠ›ã®æœŸå¾…å€¤ã‚’å¤§ããã—ã¦ã€ã¤ã¾ã‚ŠWassersteinè·é›¢ã®ç¬¬äºŒé …ã‚’å¤§ããã—ã¦ã€Wassersteinè·é›¢å°ã•ãã—ãŸã„)
            # ã“ã“ã«ç›¸é–¢ä¿‚æ•°ã‚’å°ã•ãã™ã‚‹ãƒ­ã‚¹ã‚‚åŠ ãˆã‚‹ï¼Ÿ
            if opt.withCorr:
                loss_G = -torch.mean(discriminator(hat_epsilon, is_from_generator=True)) + corr_weight*corr(hat_epsilon)
            else:
                loss_G = -torch.mean(discriminator(hat_epsilon, is_from_generator=True))
            
            # loss_Gã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³
            loss_G.backward()
            optimizer_G.step()
            
            generator_done+=1

            print("[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]" % (epoch, opt.n_epochs, batches_done % trainMatrix.shape[0]//opt.batch_size, trainMatrix.shape[0]//opt.batch_size, loss_D.item(), loss_G.item()) )
            
        
        if batches_done % opt.sample_interval == 0:
            # ã‚‚ã—ã“ã“ã§hat_epsilonä¿å­˜ã™ã‚‹ãªã‚‰ä¿å­˜ã™ã‚‹
            # hat_epsilon[:,0].shape
            # a=hat_epsilon
            # a=a.cpu().detach().numpy()
            # plt.hist(a[:,0])
            # plt.close()
            pass
        
        batches_done += 1

    loss_D_curve.append(loss_D.item())
    loss_G_curve.append(loss_G.item())

    #ã€€validtation lossã‚‚ä»˜ã‘åŠ ãˆãŸã„
    val_hat_eps = generator(valMatrix)
    val_eps = torch.randn_like(val_hat_eps)
    if opt.withGP:
        val_loss_D = -torch.mean(discriminator(val_eps, is_from_generator=False)) + torch.mean(discriminator(val_hat_eps, is_from_generator=True)) + gradient_penalty(generated_data=val_hat_eps, real_data=val_eps, gp_weight=10)
    else:
        val_loss_D = -torch.mean(discriminator(val_eps, is_from_generator=False)) + torch.mean(discriminator(val_hat_eps, is_from_generator=True))
    val_loss_D_curve.append(val_loss_D.item())
    if opt.withCorr:
        val_loss_G  = -torch.mean(discriminator(val_hat_eps, is_from_generator=True)) + corr_weight*corr(val_hat_eps)
    else:
        val_loss_G = -torch.mean(discriminator(val_hat_eps, is_from_generator=True))
    val_loss_G_curve.append(val_loss_G.item())
    
    # validationãƒ‡ãƒ¼ã‚¿ã§generatorã®å‡ºåŠ›ã®æ­£è¦æ€§æ¤œå®šã®p-å€¤ã¨ç›¸äº’ç›¸é–¢ä¿‚æ•°ã‚’ç¢ºèªã™ã‚‹
    rnd = random.randint(0, valMatrix.shape[0]-64)
    valX = valMatrix[rnd:rnd+64]
    val_hat_epsilon = generator(valX)
    # p-å€¤
    a=val_hat_epsilon[:,0].cpu().detach().numpy()
    p_value.append(stats.shapiro(a)[1])


    if saveModel:
        if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:
            torch.save(generator.state_dict(), 'parameters/generator_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'
                       .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
            torch.save(discriminator.state_dict(), 'parameters/discriminator_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'
                       .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))

        if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:
            kde = gaussian_kde(a)
            ls = np.linspace(min(a)-np.var(a), max(a)+np.var(a) , 100)
            plt.figure(figsize=(13,8))
            plt.title("generatorã®å‡ºåŠ›ã®åˆ†å¸ƒã€€\nã€€epoch:{0}/{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withGP:{8}, withCorr:{9}"
                      .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr))+"\n p-å€¤ï¼š"+str(stats.shapiro(a)[1]))
            plt.ylabel("å¯†åº¦")
            plt.plot(ls, kde.pdf(ls) , label="$\hat\epsilon$")
            plt.plot(ls, norm.pdf(ls), label="$\mathcal{N}(0,1)$")
            plt.legend()
            plt.savefig("output-images/density_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.png"
                        .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
            plt.close()

    if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:
        plt.figure(figsize=(13,8))
        plt.title("Discriminatorã®Lossã®é·ç§»ã€€\nã€€epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withGP:{8}, withCorr:{9}"
                  .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_D_curve, label="training")
        plt.plot(val_loss_D_curve, label="validation")
        plt.legend()
        plt.savefig("loss.png")
        plt.close()

torch.save(generator.state_dict(), 'parameters/generator_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'
           .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
torch.save(discriminator.state_dict(), 'parameters/discriminator_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'
           .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))



# çµæœã®ä¿å­˜
# p-å€¤
plt.figure(figsize=(13,8))
plt.plot(p_value)
plt.title("p-å€¤ã®é·ç§»ã€€\nã€€epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withPG:{8}, withCorr:{9}".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.xlabel("epoch")
plt.ylabel("p-å€¤")
plt.savefig("output-images/p-value_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}_withGP{8}_withCorr{9}.png".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed,opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.close()


# discriminatorã®Loss
plt.figure(figsize=(13,8))
plt.title("Discriminatorã®Lossã®é·ç§»ã€€\nã€€epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withPG:{8}, withCorr{9}".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_D_curve, label="training")
plt.plot(val_loss_D_curve, label="validation")
plt.legend()
plt.savefig("output-images/loss-D-curve_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}_withGP{8}_withCorr{9}.png".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.close()


# genetaratorã®Loss
plt.figure(figsize=(13,8))
plt.title("Generatorã®Lossã®é·ç§»ã€€\nã€€epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withPG:{8}, withCorr{9}".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_G_curve, label="training")
plt.plot(val_loss_G_curve, label="validation")
plt.legend()
plt.savefig("output-images/loss-G-curve_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}_withGP{8}_whithCorr{9}.png".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.close()


fig = plt.figure(figsize=(23,10))
ax1 = fig.add_subplot(111)
sm.graphics.tsa.plot_acf(generator(valMatrix).view(-1).detach().cpu().numpy(), lags=50, ax=ax1)
plt.title("Generatorã®å‡ºåŠ›ã®acfã€€\nã€€epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withPG:{8}, withCorr{9}".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.savefig("output-images/acf_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}_withGP{8}_withCorr{9}.png".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))
plt.close()