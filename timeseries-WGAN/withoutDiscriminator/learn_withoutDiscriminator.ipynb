{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triple networkから次、標準正規分布と経験分布のWasserstein距離の理論計算をLossにしてGeneratorを学習させるぜ\n",
    "これが本当のマルチタスクラーニング！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実験No.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path=path[:path.find('timeseries-WGAN')+15]\n",
    "# No = (os.path.basename(__file__))[-4]\n",
    "No = str(0)\n",
    "print('実験No.'+No)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')# 警告を非表示\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(path+\"/\")\n",
    "import random\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import japanize_matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# 人工データを生成してくれる機械が置いてあるところ\n",
    "import tsModel\n",
    "# 学習用のニューラルネットが置いてあるところ\n",
    "import models\n",
    "# p-Wasserstein距離の関数\n",
    "import Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output-images\"フォルダを作成（既にあるならそれで良し）\n",
    "os.makedirs(\"output-images\", exist_ok=True)\n",
    "os.makedirs(\"parameters\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真のモデルのパラメータ\n",
    "phi_ast=[0.3,-0.4,0.2,-0.5,0.6,-0.1,0.1]\n",
    "p_ast=len(phi_ast)\n",
    "mu_ast=0\n",
    "sigma_ast=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "trainT=1000\n",
    "n=100\n",
    "data_index = range(n)\n",
    "trainDataSets=[]\n",
    "for seed in data_index:\n",
    "    trainData = tsModel.SARIMA(a=phi_ast, N=trainT, random_seed=seed, mu=mu_ast, sigma=sigma_ast)\n",
    "    trainDataSets.append(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, data_seed=0, discriminator_hidden_unit=64, discriminator_seed=0, generator_seed=0, lr=5e-05, n_critic=5, n_epochs=10, predictor_seed=0, sample_interval=1000, training_seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--generator_seed GENERATOR_SEED]\n",
      "                             [--discriminator_seed DISCRIMINATOR_SEED]\n",
      "                             [--predictor_seed PREDICTOR_SEED]\n",
      "                             [--training_seed TRAINING_SEED]\n",
      "                             [--data_seed DATA_SEED] [--n_epochs N_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--n_critic N_CRITIC]\n",
      "                             [--discriminator_hidden_unit DISCRIMINATOR_HIDDEN_UNIT]\n",
      "                             [--sample_interval SAMPLE_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1165/jupyter/kernel-0b44adcc-8b08-47c2-ad89-0ce41db06065.json\n"
     ]
    }
   ],
   "source": [
    "# 学習する推定モデルの形状や学習方法なんかを決定します\n",
    "# 学習時のハイパラの決定（入力を受け付ける）\n",
    "parser = argparse.ArgumentParser()\n",
    "# ランダムシードについて\n",
    "parser.add_argument(\"--generator_seed\", type=int, default=0, help=\"generatorのパラメータの初期値のシード\")\n",
    "parser.add_argument(\"--discriminator_seed\", type=int, default=0, help=\"discriminatorのパラメータの初期値のシード\")\n",
    "parser.add_argument(\"--predictor_seed\", type=int, default=0, help=\"predictorのパラメータの初期値のシード\")\n",
    "parser.add_argument(\"--training_seed\", type=int, default=0, help=\"訓練データを学習させる順番を決めるシード\")\n",
    "parser.add_argument(\"--data_seed\", type=int, default=0, help=\"Dataの作成に用いる乱数のseed\")\n",
    "# 学習方法について\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=2000, help=\"Discriminatorを学習させる回数\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batchの大きさ\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"学習率\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"一度generatorを更新するごとに何回discriminatorを更新するか\")\n",
    "parser.add_argument(\"--discriminator_hidden_unit\", type=int, default=64, help=\"discriminatorの隠れ層のニューロンの数\")\n",
    "# parser.add_argument(\"--withGP\", type=bool, default=True, help=\"clipingの代わりにGradient Penaltyを加えるかどうか。True/False\")\n",
    "# parser.add_argument(\"--withCorr\", type=bool, default=True, help=\"Generatorの出力がbatch方向に無相関になるようなロスを加えるかどうか。　True/False\")\n",
    "# モデルの保存やLossの可視化について\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=1000, help=\"epochを何回まわす度にモデルの保存を行うか\")\n",
    "\n",
    "try:\n",
    "    opt = parser.parse_args() # .pyの場合はこちらを使用(.ipynbの場合エラーになります)\n",
    "except:\n",
    "    opt = parser.parse_args(args=[]) # .ipynbの場合はこちらを使用\n",
    "\n",
    "opt.n_epochs=10    \n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUが使えます。\n",
      "GPUを使いますか？ （Yes：1, No：0）  ----> 0\n"
     ]
    }
   ],
   "source": [
    "# gpuが使えるかどうか\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    print(\"GPUが使えます。\")\n",
    "    use_gpu = input('GPUを使いますか？ （Yes：1, No：0）  ----> ')\n",
    "    cuda = bool(int(use_gpu))\n",
    "else:\n",
    "    print(\"GPUは使えません。\")\n",
    "if cuda:\n",
    "    gpu_id = input('使用するGPUの番号を入れてください : ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "device = torch.device('cuda:'+gpu_id if cuda else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推定モデルの決定\n",
    "p=7\n",
    "\n",
    "os.makedirs(\"output-images/p{0}\".format(p), exist_ok=True)\n",
    "os.makedirs(\"parameters/p{0}\".format(p), exist_ok=True)\n",
    "\n",
    "torch.manual_seed(opt.generator_seed)\n",
    "generator = models.LinearGenerator(p = p, input_dim=1, is_bias=False)\n",
    "torch.manual_seed(opt.discriminator_seed)\n",
    "discriminator = models.Discriminator(q=0, discriminator_hidden_unit=opt.discriminator_hidden_unit)\n",
    "torch.manual_seed(opt.predictor_seed)\n",
    "predictor = models.LinearPredictNet(p=p, input_dim=1, is_bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データを一つ用いて学習させる\n",
    "dataSeed=opt.data_seed\n",
    "# こいつをtrain:validation=900:100に分割する\n",
    "Data = trainDataSets[dataSeed]\n",
    "Data = torch.tensor(Data, dtype=torch.float)\n",
    "Data=Data.view(1,-1)\n",
    "trainData = Data[:,:900]\n",
    "valData = Data[:,900:]\n",
    "# trainDataとvalDataを {𝑋𝑡}𝑡0𝑡=𝑡0−𝑝 ごとに取り出しやすいようにMatrixに変換する\n",
    "trainMatrix = []\n",
    "for i in range(trainData.shape[1]-(p)):\n",
    "    ans = trainData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    trainMatrix.append(ans)\n",
    "trainMatrix = torch.cat(trainMatrix)\n",
    "valMatrix = []\n",
    "for i in range(valData.shape[1]-(p)):\n",
    "    ans = valData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    valMatrix.append(ans)\n",
    "valMatrix = torch.cat(valMatrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers(パラメータに対して定義される)\n",
    "optimizer_G = torch.optim.RMSprop(params=generator.parameters(), lr=opt.lr)\n",
    "# optimizer_D = torch.optim.RMSprop(params=discriminator.parameters(), lr=opt.lr)\n",
    "optimizer_F = torch.optim.RMSprop(params=predictor.parameters(), lr=opt.lr)\n",
    "# optimizer_F = torch.optim.Adam(params=predictor.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二条誤差MSE\n",
    "mseLoss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータと学習データをGPUに乗っける\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "predictor.to(device)\n",
    "\n",
    "trainMatrix=trainMatrix.to(device)\n",
    "valMatrix=valMatrix.to(device)\n",
    "mseLoss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作成したモデルを1000epochごとに逐次保存しますか？ （Yes：1, No：0）  ----> 0\n"
     ]
    }
   ],
   "source": [
    "saveModel = input('作成したモデルを{0}epochごとに逐次保存しますか？ （Yes：1, No：0）  ----> '.format(opt.sample_interval))\n",
    "saveModel = bool(int(saveModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = 0\n",
    "generator_done = 0# generatorを学習した回数\n",
    "predictor_done = 0\n",
    "\n",
    "# グラフ描画用\n",
    "loss_D_curve = []\n",
    "loss_G_curve = []\n",
    "loss_F_curve = []\n",
    "val_loss_D_curve = []\n",
    "val_loss_G_curve = []\n",
    "val_loss_F_curve = []\n",
    "p_value = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事前学習をここで行いますか、それとも読み込みますか （行う：1, 読み込む：0）  ----> 0\n"
     ]
    }
   ],
   "source": [
    "# 訓練データの時系列のどの時刻を学習に用いるかをランダムにしているが、そのランダムシードを固定する\n",
    "random.seed(a=opt.training_seed)\n",
    "\n",
    "do_preTrain = bool(int(input('事前学習をここで行いますか、それとも読み込みますか （行う：1, 読み込む：0）  ----> ')))\n",
    "pretrain_param = 'parameters/p{0}/No{1}_predictor_epoch{2}_batchSize{3}_DataSeed{4}.pth'.format(p, No, 0, opt.batch_size, dataSeed )\n",
    "if not do_preTrain:\n",
    "    try:# モデルパラメータを読み込もうとして失敗したら、それはファイルがないと言うことなので、事前学習をこの場で行う\n",
    "        predictor.load_state_dict(torch.load(pretrain_param)) \n",
    "    except:\n",
    "        print(\"モデルが存在しないので事前学習を行います\")\n",
    "        do_preTrain=True\n",
    "\n",
    "if do_preTrain:\n",
    "    # ここでまずはFの事前学習を行う\n",
    "    loss_pre = []\n",
    "    pretrain_epoch = 1000\n",
    "    start=time.time()\n",
    "    for epoch in range(1, pretrain_epoch+1):# epochごとの処理\n",
    "        # batchの処理は、0~892をランダムに並び替えたリストbatch_sampleを作成し、ここからbatch×(p+1)の学習データを一つづつ獲得する\n",
    "        l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "        batch_sample = random.sample(l, len(l))\n",
    "        for i, batch in enumerate(batch_sample):\n",
    "            predictor.zero_grad()\n",
    "            X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "            input_tensor = X[:,:,:-1]\n",
    "            input_tensor = torch.cat([torch.randn([opt.batch_size,1,1]).to(device), input_tensor], dim=2)\n",
    "            true_tensor = X[:,:,-1]\n",
    "            prediction = predictor(input_tensor)\n",
    "            loss_F = mseLoss(prediction, true_tensor)\n",
    "            loss_F.backward()\n",
    "            optimizer_F.step()\n",
    "        loss_pre.append(loss_F.item())\n",
    "        print(\"epoch：{0}/{1}   loss_F：{2: .4f}   経過時間：{3: .1f}秒\".format(epoch, pretrain_epoch, round(loss_F.item(), 4), time.time()-start))\n",
    "        if epoch % 100==0:\n",
    "            plt.figure(figsize=(13,8))\n",
    "            plt.title(\"PredictorのLossの遷移　\\n　batchSize:{0}, GPの係数:{1}, Corrの係数:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.plot(loss_pre, label=\"training\")\n",
    "            # plt.plot(val_loss_F_curve, label=\"validation\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"preloss.png\")\n",
    "            plt.close()\n",
    "        break\n",
    "    torch.save(predictor.state_dict(), pretrain_param)\n",
    "    print(\"pre-training終了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hat_sigmaに相当する部分がほとんどになってるので1にする\n",
    "predictor.fc1.weight.data[0][0] = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch：1/10   batch：829/829   loss_G： 1.2641   loss_F： 4.6599   経過時間： 36.4秒\n",
      "validationのflossの最小値を更新しました　　Loss: 23.517175674438477\n"
     ]
    }
   ],
   "source": [
    "min_floss=np.inf# epochのflossのの最小値を保管\n",
    "start=time.time()\n",
    "for epoch in range(1, opt.n_epochs+1):# epochごとの処理(discriminatorのepoch)\n",
    "    \n",
    "    # epochごとにbatchで計算したlossを平均した値をloss_curveとして描きたい\n",
    "#     loss_D_list = []\n",
    "    loss_G_list = []\n",
    "    loss_F_list = []\n",
    "    \n",
    "    # batchの処理は、0~892をランダムに並び替えたリストbatch_sampleを作成し、ここからbatch×(p+1)の学習データを一つづつ獲得する\n",
    "    l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "    batch_sample = random.sample(l, len(l))\n",
    "    for i, batch in enumerate(batch_sample):\n",
    "        \n",
    "        X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "        \n",
    "      \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "\n",
    "        # generatorの勾配情報を0に初期化\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 現在＋過去p時刻分の時系列をgeneratorで変換した値を取得(もう一回取得する必要があるのかな...?)\n",
    "        hat_normeps_t = generator(X)# torch.Size([64, 4])\n",
    "        \n",
    "        # Adversarial loss(Wasserstein距離小さくしたい)\n",
    "        loss_G = Variable(Wasserstein.pWasserstein(hat_normeps_t.view(opt.batch_size), p=1), requires_grad=True).to(device)\n",
    "        loss_G_list.append(float(loss_G))\n",
    "        # loss_Gを目的関数としてネットワークの全パラメータで微分をしてくれと言う合図\n",
    "        loss_G.backward()\n",
    "        # generatorのパラメータをその微分値とoptimizerを使って更新してくれ！\n",
    "        optimizer_G.step()\n",
    "\n",
    "        generator_done+=1\n",
    "\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Predictor\n",
    "        # -----------------\n",
    "        \n",
    "        # discriminatorをopt.n_critic回学習させるごとに一回predictorを学習させる(ただし最初はめっちゃdiscriminatorを優先させる)\n",
    "        # if batches_done % (1000 if predictor_done<25 or predictor_done%500==0 else opt.n_critic) == 0:\n",
    "        if batches_done % 1000 == 0:        \n",
    "            # generatorとpredictorの勾配情報を0に初期化(一応。前回の更新時の勾配情報をなくすため？)\n",
    "            optimizer_G.zero_grad()\n",
    "            optimizer_F.zero_grad()\n",
    "            \n",
    "            # 正規化されたinnoationの推定量をgeneratorを用いて算出\n",
    "            hat_normeps_t = generator(X)\n",
    "            # これと過去p時刻の時系列の値（X_{t-1}, .... , X_{t-p}）をpredictorへ入力\n",
    "            input_tensor = torch.cat([hat_normeps_t.view(opt.batch_size, -1, 1), X[:,:,:-1]], dim=2)\n",
    "            prediction = predictor(input_tensor)\n",
    "            \n",
    "            loss_F = mseLoss(prediction, X[:,:,-1])\n",
    "            loss_F_list.append(float(loss_F))\n",
    "            \n",
    "            \n",
    "            loss_F.backward()\n",
    "            optimizer_F.step()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            predictor_done+=1\n",
    "        batches_done+=1\n",
    "\n",
    "    print(\"epoch：{0}/{1}   batch：{2:003}/{3}   loss_G：{4: .4f}   loss_F：{5: .4f}   経過時間：{6: .1f}秒\".format(epoch, opt.n_epochs, i+1, len(batch_sample), round(float(loss_G), 4), round(float(loss_F), 4), time.time()-start))\n",
    "            \n",
    "    if saveModel and epoch % opt.sample_interval == 0:\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "    # epochごとにbatchで計算したlossを平均した値をloss_curveとして描きたい\n",
    "#     try:\n",
    "#         loss_D_curve.append(sum(loss_D_list)/len(loss_D_list))\n",
    "#     except:\n",
    "#         loss_D_curve.append(None)\n",
    "    try:\n",
    "        loss_G_curve.append(sum(loss_G_list)/len(loss_G_list))\n",
    "    except:\n",
    "        loss_G_curve.append(None)\n",
    "    try:\n",
    "        loss_F_curve.append(sum(loss_F_list)/len(loss_F_list))\n",
    "    except:\n",
    "        loss_F_curve.append(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    # validationデータによるlossも計算したい\n",
    "    val_hat_normeps_t = generator(valMatrix)\n",
    "    val_normeps_t = torch.randn_like(val_hat_normeps_t)\n",
    "    val_input_tensor = torch.cat([val_hat_normeps_t.view(-1, 1,1), valMatrix[:,:,:-1]], dim=2)\n",
    "    \n",
    "#     val_loss_D = -torch.mean(discriminator(val_normeps_t)) + torch.mean(discriminator(val_hat_normeps_t))\n",
    "#     if opt.withGP:\n",
    "#         val_loss_D = val_loss_D + gradient_penalty(generated_data=val_hat_normeps_t, real_data=val_normeps_t, gp_weight=gp_weight) \n",
    "#     val_loss_D_curve.append(float(val_loss_D))\n",
    "    val_loss_G = Wasserstein.pWasserstein(val_hat_normeps_t.view(-1), p=1)\n",
    "    # if opt.withCorr:\n",
    "    #     val_loss_G = val_loss_G + corr_weight*corr(val_hat_normeps_t)\n",
    "    val_loss_G_curve.append(float(val_loss_G))\n",
    "    \n",
    "    val_loss_F = mseLoss(predictor(val_input_tensor), valMatrix[:,:,0])\n",
    "    val_loss_F_curve.append(float(val_loss_F))\n",
    "\n",
    "    # val_loss_Fの最小値を保管\n",
    "    if min_floss > val_loss_F_curve[-1]:\n",
    "        min_floss=val_loss_F_curve[-1]\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "        print(\"validationのflossの最小値を更新しました　　Loss:\", min_floss)\n",
    "    \n",
    "    if epoch % 10==0:\n",
    "#         plt.figure(figsize=(13,8))\n",
    "#         plt.title(\"DiscriminatorのLossの遷移　\\n　batchSize:{0}, GPの係数:{1}, Corrの係数:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.plot(loss_D_curve, label=\"training\")\n",
    "#         plt.plot(val_loss_D_curve, label=\"validation\")\n",
    "#         plt.legend()\n",
    "#         plt.savefig(\"dloss.png\")\n",
    "#         plt.close()\n",
    "\n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"GeneratorのLossの遷移　\\n　batchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_G_curve, label=\"training\")\n",
    "        plt.plot(val_loss_G_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"gloss.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"PredictorのLossの遷移　\\n　batchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_F_curve, label=\"training\")\n",
    "        plt.plot(val_loss_F_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"floss.png\")\n",
    "        plt.close()\n",
    "    break\n",
    "torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(13,8))\n",
    "# plt.title(\"DiscriminatorのLossの遷移　\\n　batchSize:{0}, GPの係数:{1}, Corrの係数:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.plot(loss_D_curve, label=\"training\")\n",
    "# plt.plot(val_loss_D_curve, label=\"validation\")\n",
    "# plt.legend()\n",
    "# plt.savefig(\"output-images/p{0}/No{6}_dloss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png\".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed, No ))\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.title(\"GeneratorのLossの遷移　\\n　batchSize:{0}, GPの係数:{1}, Corrの係数:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_G_curve, label=\"training\")\n",
    "plt.plot(val_loss_G_curve, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.savefig(\"output-images/p{0}/No{6}_gloss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png\".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed, No ))\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(13,8))\n",
    "plt.title(\"PredictorのLossの遷移　\\n　batchSize:{0}, GPの係数:{1}, Corrの係数:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_F_curve, label=\"training\")\n",
    "plt.plot(val_loss_F_curve, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.savefig(\"output-images/p{0}/No{6}_floss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png\".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed, No ))\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
