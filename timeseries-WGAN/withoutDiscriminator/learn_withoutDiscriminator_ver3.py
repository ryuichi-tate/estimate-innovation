'''
triple networkã‹ã‚‰æ¬¡ã€æ¨™æº–æ­£è¦åˆ†å¸ƒã¨çµŒé¨“åˆ†å¸ƒã®Wassersteinè·é›¢ã®ç†è«–è¨ˆç®—ã‚’Lossã«ã—ã¦Generatorã‚’å­¦ç¿’ã•ã›ã‚‹ãœ
ã“ã‚ŒãŒæœ¬å½“ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼
'''
import argparse
import os
path = os.getcwd()
path=path[:path.find('timeseries-WGAN')+15]
No = (os.path.basename(__file__))[-4]
# No = str(0) # notebookç”¨
print('å®Ÿé¨“No.'+No)
import warnings
warnings.simplefilter('ignore')# è­¦å‘Šã‚’éè¡¨ç¤º
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import math
import sys
sys.path.append(path+"/")
import random
import time
import statsmodels.api as sm
from scipy.stats import norm
import japanize_matplotlib
from scipy.stats import gaussian_kde

import torchvision.transforms as transforms
from torchvision.utils import save_image

import torch.nn as nn
import torch.nn.functional as F
import torch
from torch.autograd import Variable
# äººå·¥ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹æ©Ÿæ¢°ãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import tsModel
# å­¦ç¿’ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import models
# p-Wassersteinè·é›¢ã®é–¢æ•°
import Wasserstein

# "output-images"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«ã‚ã‚‹ãªã‚‰ãã‚Œã§è‰¯ã—ï¼‰
os.makedirs("output-images", exist_ok=True)
os.makedirs("parameters", exist_ok=True)

# çœŸã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
phi_ast=[0.3,-0.4,0.2,-0.5,0.6,-0.1,0.1]
p_ast=len(phi_ast)
mu_ast=0
sigma_ast=2

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
trainT=1000
n=100
data_index = range(n)
trainDataSets=[]
for seed in data_index:
    trainData = tsModel.SARIMA(a=phi_ast, N=trainT, random_seed=seed, mu=mu_ast, sigma=sigma_ast)
    trainDataSets.append(trainData)

# å­¦ç¿’ã™ã‚‹æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å½¢çŠ¶ã‚„å­¦ç¿’æ–¹æ³•ãªã‚“ã‹ã‚’æ±ºå®šã—ã¾ã™
# å­¦ç¿’æ™‚ã®ãƒã‚¤ãƒ‘ãƒ©ã®æ±ºå®šï¼ˆå…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ï¼‰
parser = argparse.ArgumentParser()
# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã¤ã„ã¦
parser.add_argument("--generator_seed", type=int, default=0, help="generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--discriminator_seed", type=int, default=0, help="discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--predictor_seed", type=int, default=0, help="predictorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--training_seed", type=int, default=0, help="è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã‚‹é †ç•ªã‚’æ±ºã‚ã‚‹ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--data_seed", type=int, default=0, help="Dataã®ä½œæˆã«ç”¨ã„ã‚‹ä¹±æ•°ã®seed")
# å­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦
parser.add_argument("--n_epochs", type=int, default=2000, help="Discriminatorã‚’å­¦ç¿’ã•ã›ã‚‹å›æ•°")
parser.add_argument("--batch_size", type=int, default=64, help="batchã®å¤§ãã•")
parser.add_argument("--lr", type=float, default=0.00005, help="å­¦ç¿’ç‡")
parser.add_argument("--n_critic", type=int, default=5, help="ä¸€åº¦generatorã‚’æ›´æ–°ã™ã‚‹ã”ã¨ã«ä½•å›discriminatorã‚’æ›´æ–°ã™ã‚‹ã‹")
parser.add_argument("--discriminator_hidden_unit", type=int, default=64, help="discriminatorã®éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•°")
# parser.add_argument("--withGP", type=bool, default=True, help="clipingã®ä»£ã‚ã‚Šã«Gradient Penaltyã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚True/False")
# parser.add_argument("--withCorr", type=bool, default=True, help="Generatorã®å‡ºåŠ›ãŒbatchæ–¹å‘ã«ç„¡ç›¸é–¢ã«ãªã‚‹ã‚ˆã†ãªãƒ­ã‚¹ã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚ã€€True/False")
# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚„Lossã®å¯è¦–åŒ–ã«ã¤ã„ã¦
parser.add_argument("--sample_interval", type=int, default=100, help="epochã‚’ä½•å›ã¾ã‚ã™åº¦ã«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚’è¡Œã†ã‹")

try:
    opt = parser.parse_args() # .pyã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨(.ipynbã®å ´åˆã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™)
except:
    opt = parser.parse_args(args=[]) # .ipynbã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨

print(opt)

# gpuãŒä½¿ãˆã‚‹ã‹ã©ã†ã‹
cuda = True if torch.cuda.is_available() else False
if cuda:
    print("GPUãŒä½¿ãˆã¾ã™ã€‚")
    use_gpu = input('GPUã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> ')
    cuda = bool(int(use_gpu))
else:
    print("GPUã¯ä½¿ãˆã¾ã›ã‚“ã€‚")
if cuda:
    gpu_id = input('ä½¿ç”¨ã™ã‚‹GPUã®ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ : ')
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
device = torch.device('cuda:'+gpu_id if cuda else 'cpu')

# æ¨å®šãƒ¢ãƒ‡ãƒ«ã®æ±ºå®š
p=7

os.makedirs("output-images/p{0}".format(p), exist_ok=True)
os.makedirs("parameters/p{0}".format(p), exist_ok=True)

torch.manual_seed(opt.generator_seed)
generator = models.LinearGenerator(p = p, input_dim=1, is_bias=False)
torch.manual_seed(opt.predictor_seed)
predictor = models.LinearPredictNet(p=p, input_dim=1, is_bias=True)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ç”¨ã„ã¦å­¦ç¿’ã•ã›ã‚‹
dataSeed=opt.data_seed
# ã“ã„ã¤ã‚’train:validation=900:100ã«åˆ†å‰²ã™ã‚‹
Data = trainDataSets[dataSeed]
Data = torch.tensor(Data, dtype=torch.float)
Data=Data.view(1,-1)
trainData = Data[:,:900]
valData = Data[:,900:]
# trainDataã¨valDataã‚’ {ğ‘‹ğ‘¡}ğ‘¡0ğ‘¡=ğ‘¡0âˆ’ğ‘ ã”ã¨ã«å–ã‚Šå‡ºã—ã‚„ã™ã„ã‚ˆã†ã«Matrixã«å¤‰æ›ã™ã‚‹
trainMatrix = []
for i in range(trainData.shape[1]-(p)):
    ans = trainData[:,i:i+p+1].view(1,Data.shape[0],-1)
    trainMatrix.append(ans)
trainMatrix = torch.cat(trainMatrix)
valMatrix = []
for i in range(valData.shape[1]-(p)):
    ans = valData[:,i:i+p+1].view(1,Data.shape[0],-1)
    valMatrix.append(ans)
valMatrix = torch.cat(valMatrix)

# Optimizers(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹)
optimizer_G = torch.optim.RMSprop(params=generator.parameters(), lr=opt.lr)
# optimizer_D = torch.optim.RMSprop(params=discriminator.parameters(), lr=opt.lr)
optimizer_F = torch.optim.RMSprop(params=predictor.parameters(), lr=opt.lr)
# optimizer_F = torch.optim.Adam(params=predictor.parameters())

# äºŒæ¡èª¤å·®MSE
mseLoss = nn.MSELoss()

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ä¹—ã£ã‘ã‚‹
generator.to(device)
# discriminator.to(device)
predictor.to(device)
trainMatrix=trainMatrix.to(device)
valMatrix=valMatrix.to(device)
mseLoss.to(device)

saveModel = input('ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ {0} epochã”ã¨ã«é€æ¬¡ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> '.format(opt.sample_interval))
saveModel = bool(int(saveModel))


# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ã®ã©ã®æ™‚åˆ»ã‚’å­¦ç¿’ã«ç”¨ã„ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ã„ã‚‹ãŒã€ãã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã™ã‚‹
random.seed(a=opt.training_seed)

do_preTrain = bool(int(input('äº‹å‰å­¦ç¿’ã‚’ã“ã“ã§è¡Œã„ã¾ã™ã‹ã€ãã‚Œã¨ã‚‚èª­ã¿è¾¼ã¿ã¾ã™ã‹ ï¼ˆè¡Œã†ï¼š1, èª­ã¿è¾¼ã‚€ï¼š0ï¼‰  ----> ')))
pretrain_param = 'parameters/p{0}/No{1}_predictor_epoch{2}_batchSize{3}_DataSeed{4}.pth'.format(p, No, 0, opt.batch_size, dataSeed )
if not do_preTrain:
    try:# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¦å¤±æ•—ã—ãŸã‚‰ã€ãã‚Œã¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ã¨è¨€ã†ã“ã¨ãªã®ã§ã€äº‹å‰å­¦ç¿’ã‚’ã“ã®å ´ã§è¡Œã†
        predictor.load_state_dict(torch.load(pretrain_param)) 
    except:
        print("ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ã®ã§äº‹å‰å­¦ç¿’ã‚’è¡Œã„ã¾ã™")
        do_preTrain=True

if do_preTrain:
    # ã“ã“ã§ã¾ãšã¯Fã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†
    loss_pre = []
    pretrain_epoch = 1000
    start=time.time()
    for epoch in range(1, pretrain_epoch+1):# epochã”ã¨ã®å‡¦ç†
        # batchã®å‡¦ç†ã¯ã€0~892ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆãŸãƒªã‚¹ãƒˆbatch_sampleã‚’ä½œæˆã—ã€ã“ã“ã‹ã‚‰batchÃ—(p+1)ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã¥ã¤ç²å¾—ã™ã‚‹
        l=list(range(trainMatrix.shape[0]-opt.batch_size))
        batch_sample = random.sample(l, len(l))
        for i, batch in enumerate(batch_sample):
            X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)
            optimizer_F.zero_grad()
            input_tensor = X[:,:,:-1]
            input_tensor = torch.cat([torch.randn([opt.batch_size,1,1]).to(device), input_tensor], dim=2)
            true_tensor = X[:,:,-1]
            prediction = predictor(input_tensor)
            loss_F = mseLoss(prediction, true_tensor)
            loss_F.backward()
            optimizer_F.step()
        loss_pre.append(loss_F.item())
        print("epochï¼š{0}/{1}   loss_Fï¼š{2: .4f}   çµŒéæ™‚é–“ï¼š{3: .1f}ç§’".format(epoch, pretrain_epoch, round(loss_F.item(), 4), time.time()-start))
        if epoch % 100==0:
            plt.figure(figsize=(13,8))
            plt.title("Predictorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}".format(opt.batch_size))
            plt.xlabel("epoch")
            plt.ylabel("Loss")
            plt.plot(loss_pre, label="training")
            # plt.plot(val_loss_F_curve, label="validation")
            plt.legend()
            plt.savefig("preloss.png")
            plt.close()
    torch.save(predictor.state_dict(), pretrain_param)
    print("pre-trainingçµ‚äº†")

# hat_sigmaã«ç›¸å½“ã™ã‚‹éƒ¨åˆ†ãŒã»ã¨ã‚“ã©ã«ãªã£ã¦ã‚‹ã®ã§1ã«ã™ã‚‹
predictor.fc1.weight.data[0][0] = torch.tensor(1)

min_floss=np.inf# epochã®flossã®ã®æœ€å°å€¤ã‚’ä¿ç®¡
start=time.time()

batches_done = 0
epoch_done = 0# generatorã‚’å­¦ç¿’ã—ãŸå›æ•°
loss_curve = []

for epoch in range(1, opt.n_epochs+1):# epochã”ã¨ã®å‡¦ç†(discriminatorã®epoch)
    
    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„
    loss_list = []
    
    # batchã®å‡¦ç†ã¯ã€0~892ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆãŸãƒªã‚¹ãƒˆbatch_sampleã‚’ä½œæˆã—ã€ã“ã“ã‹ã‚‰batchÃ—(p+1)ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã¥ã¤ç²å¾—ã™ã‚‹
    l=list(range(trainMatrix.shape[0]-opt.batch_size))
    batch_sample = random.sample(l, len(l))
    for i, batch in enumerate(batch_sample):
        
        X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)

        # generatorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–
        optimizer_F.zero_grad()
        optimizer_G.zero_grad()

        # æ­£è¦åŒ–ã•ã‚ŒãŸinnoationã®æ¨å®šé‡ã‚’generatorã‚’ç”¨ã„ã¦ç®—å‡º
        hat_normeps_t = generator(X)
        # ã“ã‚Œã¨éå»pæ™‚åˆ»ã®æ™‚ç³»åˆ—ã®å€¤ï¼ˆX_{t-1}, .... , X_{t-p}ï¼‰ã‚’predictorã¸å…¥åŠ›
        input_tensor = torch.cat([hat_normeps_t.view(opt.batch_size, -1, 1), X[:,:,:-1]], dim=2)
        prediction = predictor(input_tensor)

        loss_G = Variable(Wasserstein.pWasserstein(hat_normeps_t.view(opt.batch_size), p=1), requires_grad=True).to(device)
        loss_F = mseLoss(prediction, X[:,:,-1])

        loss = loss_G+loss_F
        loss_list.append(loss.item())

        # lossã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³
        loss.backward()
        # generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã®å¾®åˆ†å€¤ã¨optimizerã‚’ä½¿ã£ã¦æ›´æ–°ã—ã¦ãã‚Œï¼
        optimizer_G.step()
        optimizer_F.step()

        generator_done+=1


    print("epochï¼š{0}/{1}   batchï¼š{2:003}/{3}   loss_Gï¼š{4: .4f}   loss_Fï¼š{5: .4f}   çµŒéæ™‚é–“ï¼š{6: .1f}ç§’".format(epoch, opt.n_epochs, i+1, len(batch_sample), round(float(loss_G), 4), round(float(loss_F), 4), time.time()-start))
            
    if saveModel and epoch % opt.sample_interval == 0:
        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))
        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))

    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„
#     try:
#         loss_D_curve.append(sum(loss_D_list)/len(loss_D_list))
#     except:
#         loss_D_curve.append(None)
    try:
        loss_G_curve.append(sum(loss_G_list)/len(loss_G_list))
    except:
        loss_G_curve.append(None)
    try:
        loss_F_curve.append(sum(loss_F_list)/len(loss_F_list))
    except:
        loss_F_curve.append(None)
    

    
    # validationãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹lossã‚‚è¨ˆç®—ã—ãŸã„
    val_hat_normeps_t = generator(valMatrix)
    val_normeps_t = torch.randn_like(val_hat_normeps_t)
    val_input_tensor = torch.cat([val_hat_normeps_t.view(-1, 1,1), valMatrix[:,:,:-1]], dim=2)
    
#     val_loss_D = -torch.mean(discriminator(val_normeps_t)) + torch.mean(discriminator(val_hat_normeps_t))
#     if opt.withGP:
#         val_loss_D = val_loss_D + gradient_penalty(generated_data=val_hat_normeps_t, real_data=val_normeps_t, gp_weight=gp_weight) 
#     val_loss_D_curve.append(float(val_loss_D))
    val_loss_G = Wasserstein.pWasserstein(val_hat_normeps_t.view(-1), p=1)
    # if opt.withCorr:
    #     val_loss_G = val_loss_G + corr_weight*corr(val_hat_normeps_t)
    val_loss_G_curve.append(float(val_loss_G))
    
    val_loss_F = mseLoss(predictor(val_input_tensor), valMatrix[:,:,0])
    val_loss_F_curve.append(float(val_loss_F))

    # val_loss_Fã®æœ€å°å€¤ã‚’ä¿ç®¡
    if min_floss > val_loss_F_curve[-1]:
        min_floss=val_loss_F_curve[-1]
        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))
        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))

        print("validationã®flossã®æœ€å°å€¤ã‚’æ›´æ–°ã—ã¾ã—ãŸã€€ã€€Loss:", min_floss)
    
    if epoch % 10==0:
#         plt.figure(figsize=(13,8))
#         plt.title("Discriminatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
#         plt.xlabel("epoch")
#         plt.ylabel("Loss")
#         plt.plot(loss_D_curve, label="training")
#         plt.plot(val_loss_D_curve, label="validation")
#         plt.legend()
#         plt.savefig("dloss.png")
#         plt.close()

        plt.figure(figsize=(13,8))
        plt.title("Generatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}".format(opt.batch_size))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_G_curve, label="training")
        plt.plot(val_loss_G_curve, label="validation")
        plt.legend()
        plt.savefig("gloss.png")
        plt.close()
        
        plt.figure(figsize=(13,8))
        plt.title("Predictorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}".format(opt.batch_size))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_F_curve, label="training")
        plt.plot(val_loss_F_curve, label="validation")
        plt.legend()
        plt.savefig("floss.png")
        plt.close()
    
torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))
torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))


plt.figure(figsize=(13,8))
plt.title("Generatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}".format(opt.batch_size))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_G_curve, label="training")
plt.plot(val_loss_G_curve, label="validation")
plt.legend()
plt.savefig("output-images/p{0}/No{1}_gloss_epoch{2}_batchSize{3}_DataSeed{4}.png".format(p, No, epoch, opt.batch_size, dataSeed ))
# plt.show()
plt.close()

plt.figure(figsize=(13,8))
plt.title("Predictorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}".format(opt.batch_size))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_F_curve, label="training")
plt.plot(val_loss_F_curve, label="validation")
plt.legend()
plt.savefig("output-images/p{0}/No{1}_floss_epoch{2}_batchSize{3}_DataSeed{4}.png".format(p, No, epoch, opt.batch_size, dataSeed ))
# plt.show()
plt.close()