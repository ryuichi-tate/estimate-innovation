{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Ÿé¨“No.0_3\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path=path[:path.find('timeseries-WGAN')+15]\n",
    "# No = (os.path.basename(__file__))[-4]\n",
    "No = \"0_3\" # notebookç”¨\n",
    "print('å®Ÿé¨“No.'+No)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')# è­¦å‘Šã‚’éè¡¨ç¤º\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(path+\"/\")\n",
    "import random\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import japanize_matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# äººå·¥ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹æ©Ÿæ¢°ãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚\n",
    "import tsModel\n",
    "# å­¦ç¿’ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚\n",
    "import models\n",
    "# p-Wassersteinè·é›¢ã®é–¢æ•°\n",
    "import Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output-images\"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«ã‚ã‚‹ãªã‚‰ãã‚Œã§è‰¯ã—ï¼‰\n",
    "os.makedirs(\"output-images\", exist_ok=True)\n",
    "os.makedirs(\"parameters\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çœŸã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "phi_ast=[0.3,-0.4,0.2,-0.5,0.6,-0.1,0.1]\n",
    "p_ast=len(phi_ast)\n",
    "mu_ast=0\n",
    "sigma_ast=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "trainT=1000\n",
    "n=100\n",
    "data_index = range(n)\n",
    "trainDataSets=[]\n",
    "for seed in data_index:\n",
    "    trainData = tsModel.SARIMA(a=phi_ast, N=trainT, random_seed=seed, mu=mu_ast, sigma=sigma_ast)\n",
    "    trainDataSets.append(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, data_seed=0, discriminator_hidden_unit=64, discriminator_seed=0, generator_seed=0, lr=5e-05, n_critic=5, n_epochs=2000, predictor_seed=0, sample_interval=100, training_seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--generator_seed GENERATOR_SEED]\n",
      "                             [--discriminator_seed DISCRIMINATOR_SEED]\n",
      "                             [--predictor_seed PREDICTOR_SEED]\n",
      "                             [--training_seed TRAINING_SEED]\n",
      "                             [--data_seed DATA_SEED] [--n_epochs N_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--n_critic N_CRITIC]\n",
      "                             [--discriminator_hidden_unit DISCRIMINATOR_HIDDEN_UNIT]\n",
      "                             [--sample_interval SAMPLE_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1165/jupyter/kernel-2330cc2b-2aab-479b-9155-284f76844235.json\n"
     ]
    }
   ],
   "source": [
    "# å­¦ç¿’ã™ã‚‹æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å½¢çŠ¶ã‚„å­¦ç¿’æ–¹æ³•ãªã‚“ã‹ã‚’æ±ºå®šã—ã¾ã™\n",
    "# å­¦ç¿’æ™‚ã®ãƒã‚¤ãƒ‘ãƒ©ã®æ±ºå®šï¼ˆå…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ï¼‰\n",
    "parser = argparse.ArgumentParser()\n",
    "# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã¤ã„ã¦\n",
    "parser.add_argument(\"--generator_seed\", type=int, default=0, help=\"generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰\")\n",
    "parser.add_argument(\"--discriminator_seed\", type=int, default=0, help=\"discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰\")\n",
    "parser.add_argument(\"--predictor_seed\", type=int, default=0, help=\"predictorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰\")\n",
    "parser.add_argument(\"--training_seed\", type=int, default=0, help=\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã‚‹é †ç•ªã‚’æ±ºã‚ã‚‹ã‚·ãƒ¼ãƒ‰\")\n",
    "parser.add_argument(\"--data_seed\", type=int, default=0, help=\"Dataã®ä½œæˆã«ç”¨ã„ã‚‹ä¹±æ•°ã®seed\")\n",
    "# å­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=2000, help=\"Discriminatorã‚’å­¦ç¿’ã•ã›ã‚‹å›æ•°\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batchã®å¤§ãã•\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"å­¦ç¿’ç‡\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"ä¸€åº¦generatorã‚’æ›´æ–°ã™ã‚‹ã”ã¨ã«ä½•å›discriminatorã‚’æ›´æ–°ã™ã‚‹ã‹\")\n",
    "parser.add_argument(\"--discriminator_hidden_unit\", type=int, default=64, help=\"discriminatorã®éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•°\")\n",
    "# parser.add_argument(\"--withGP\", type=bool, default=True, help=\"clipingã®ä»£ã‚ã‚Šã«Gradient Penaltyã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚True/False\")\n",
    "# parser.add_argument(\"--withCorr\", type=bool, default=True, help=\"Generatorã®å‡ºåŠ›ãŒbatchæ–¹å‘ã«ç„¡ç›¸é–¢ã«ãªã‚‹ã‚ˆã†ãªãƒ­ã‚¹ã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚ã€€True/False\")\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚„Lossã®å¯è¦–åŒ–ã«ã¤ã„ã¦\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"epochã‚’ä½•å›ã¾ã‚ã™åº¦ã«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚’è¡Œã†ã‹\")\n",
    "\n",
    "try:\n",
    "    opt = parser.parse_args() # .pyã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨(.ipynbã®å ´åˆã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™)\n",
    "except:\n",
    "    opt = parser.parse_args(args=[]) # .ipynbã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUãŒä½¿ãˆã¾ã™ã€‚\n",
      "GPUã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> 1\n",
      "ä½¿ç”¨ã™ã‚‹GPUã®ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ : 1\n"
     ]
    }
   ],
   "source": [
    "# gpuãŒä½¿ãˆã‚‹ã‹ã©ã†ã‹\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    print(\"GPUãŒä½¿ãˆã¾ã™ã€‚\")\n",
    "    use_gpu = input('GPUã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> ')\n",
    "    cuda = bool(int(use_gpu))\n",
    "else:\n",
    "    print(\"GPUã¯ä½¿ãˆã¾ã›ã‚“ã€‚\")\n",
    "if cuda:\n",
    "    gpu_id = input('ä½¿ç”¨ã™ã‚‹GPUã®ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ : ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "device = torch.device('cuda:'+gpu_id if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨å®šãƒ¢ãƒ‡ãƒ«ã®æ±ºå®š\n",
    "p=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output-images/p{0}\".format(p), exist_ok=True)\n",
    "os.makedirs(\"parameters/p{0}\".format(p), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d729405f2d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearPredictNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'p'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(opt.generator_seed)\n",
    "generator = models.LinearGenerator(p = p, input_dim=1, is_bias=False)\n",
    "torch.manual_seed(opt.predictor_seed)\n",
    "predictor = models.LinearPredictNet(p=p, input_dim=1, is_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ç”¨ã„ã¦å­¦ç¿’ã•ã›ã‚‹\n",
    "dataSeed=opt.data_seed\n",
    "# ã“ã„ã¤ã‚’train:validation=900:100ã«åˆ†å‰²ã™ã‚‹\n",
    "Data = trainDataSets[dataSeed]\n",
    "Data = torch.tensor(Data, dtype=torch.float)\n",
    "Data=Data.view(1,-1)\n",
    "trainData = Data[:,:900]\n",
    "valData = Data[:,900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDataã¨valDataã‚’ {ğ‘‹ğ‘¡}ğ‘¡0ğ‘¡=ğ‘¡0âˆ’ğ‘ ã”ã¨ã«å–ã‚Šå‡ºã—ã‚„ã™ã„ã‚ˆã†ã«Matrixã«å¤‰æ›ã™ã‚‹\n",
    "trainMatrix = []\n",
    "for i in range(trainData.shape[1]-(p)):\n",
    "    ans = trainData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    trainMatrix.append(ans)\n",
    "trainMatrix = torch.cat(trainMatrix)\n",
    "valMatrix = []\n",
    "for i in range(valData.shape[1]-(p)):\n",
    "    ans = valData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    valMatrix.append(ans)\n",
    "valMatrix = torch.cat(valMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹)\n",
    "optimizer_G = torch.optim.RMSprop(params=generator.parameters(), lr=opt.lr)\n",
    "# optimizer_D = torch.optim.RMSprop(params=discriminator.parameters(), lr=opt.lr)\n",
    "optimizer_F = torch.optim.RMSprop(params=predictor.parameters(), lr=opt.lr)\n",
    "# optimizer_F = torch.optim.Adam(params=predictor.parameters())\n",
    "\n",
    "# äºŒæ¡èª¤å·®MSE\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ä¹—ã£ã‘ã‚‹\n",
    "generator.to(device)\n",
    "# discriminator.to(device)\n",
    "predictor.to(device)\n",
    "trainMatrix=trainMatrix.to(device)\n",
    "valMatrix=valMatrix.to(device)\n",
    "mseLoss.to(device)\n",
    "\n",
    "saveModel = input('ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ {0} epochã”ã¨ã«é€æ¬¡ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> '.format(opt.sample_interval))\n",
    "saveModel = bool(int(saveModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ã®ã©ã®æ™‚åˆ»ã‚’å­¦ç¿’ã«ç”¨ã„ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ã„ã‚‹ãŒã€ãã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã™ã‚‹\n",
    "random.seed(a=opt.training_seed)\n",
    "\n",
    "do_preTrain = bool(int(input('äº‹å‰å­¦ç¿’ã‚’ã“ã“ã§è¡Œã„ã¾ã™ã‹ã€ãã‚Œã¨ã‚‚èª­ã¿è¾¼ã¿ã¾ã™ã‹ ï¼ˆè¡Œã†ï¼š1, èª­ã¿è¾¼ã‚€ï¼š0ï¼‰  ----> ')))\n",
    "pretrain_param = 'parameters/p{0}/No{1}_predictor_epoch{2}_batchSize{3}_DataSeed{4}.pth'.format(p, No, 0, opt.batch_size, dataSeed )\n",
    "if not do_preTrain:\n",
    "    try:# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚‚ã†ã¨ã—ã¦å¤±æ•—ã—ãŸã‚‰ã€ãã‚Œã¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ã¨è¨€ã†ã“ã¨ãªã®ã§ã€äº‹å‰å­¦ç¿’ã‚’ã“ã®å ´ã§è¡Œã†\n",
    "        predictor.load_state_dict(torch.load(pretrain_param)) \n",
    "    except:\n",
    "        print(\"ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ã®ã§äº‹å‰å­¦ç¿’ã‚’è¡Œã„ã¾ã™\")\n",
    "        do_preTrain=True\n",
    "\n",
    "if do_preTrain:\n",
    "    # ã“ã“ã§ã¾ãšã¯Fã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†\n",
    "    loss_pre = []\n",
    "    val_loss_pre = []\n",
    "    pretrain_epoch = 1000\n",
    "    start=time.time()\n",
    "    for epoch in range(1, pretrain_epoch+1):# epochã”ã¨ã®å‡¦ç†\n",
    "        # batchã®å‡¦ç†ã¯ã€0~892ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆãŸãƒªã‚¹ãƒˆbatch_sampleã‚’ä½œæˆã—ã€ã“ã“ã‹ã‚‰batchÃ—(p+1)ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã¥ã¤ç²å¾—ã™ã‚‹\n",
    "        l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "        batch_sample = random.sample(l, len(l))\n",
    "        for i, batch in enumerate(batch_sample):\n",
    "            X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "            optimizer_F.zero_grad()\n",
    "            input_tensor = X[:,:,:-1]\n",
    "            input_tensor = torch.cat([torch.randn([opt.batch_size,1,1]).to(device), input_tensor], dim=2)\n",
    "            true_tensor = X[:,:,-1]\n",
    "            prediction = predictor(input_tensor)\n",
    "            loss_F = mseLoss(prediction, true_tensor)\n",
    "            loss_F.backward()\n",
    "            optimizer_F.step()\n",
    "        loss_pre.append(loss_F.item())\n",
    "        \n",
    "        val_input = torch.cat([torch.randn([valMatrix.shape[0],1,1]).to(device), valMatrix[:,:,:-1]], dim=2)\n",
    "        val_target = valMatrix[:,:,-1].view(valMatrix.shape[0], -1)\n",
    "        val_loss = mseLoss(predictor(val_input), val_target)\n",
    "        val_loss_pre.append(val_loss.item())\n",
    "        \n",
    "        print(\"epochï¼š{0}/{1}   loss_Fï¼š{2: .4f}   çµŒéæ™‚é–“ï¼š{3: .1f}ç§’\".format(epoch, pretrain_epoch, round(loss_F.item(), 4), time.time()-start))\n",
    "        if epoch % 100==0:\n",
    "            plt.figure(figsize=(13,8))\n",
    "            plt.title(\"Predictorã®Lossã®é·ç§»ã€€\\nã€€batchSize:{0}\".format(opt.batch_size))\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.plot(loss_pre, label=\"training\")\n",
    "            plt.plot(val_loss_pre, label=\"validation\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"preloss.png\")\n",
    "            plt.close()\n",
    "    torch.save(predictor.state_dict(), pretrain_param)\n",
    "    \n",
    "    plt.figure(figsize=(13,8))\n",
    "    plt.title(\"Predictorã®Lossã®é·ç§»ã€€\\nã€€batchSize:{0}\".format(opt.batch_size))\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_pre, label=\"training\")\n",
    "    plt.plot(val_loss_pre, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"output-images/p{0}/No{1}_preloss_epoch{2}_batchSize{3}_DataSeed{4}.png\".format(p, No, epoch, opt.batch_size, dataSeed ))\n",
    "    plt.close()\n",
    "    print(\"pre-trainingçµ‚äº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hat_sigmaã«ç›¸å½“ã™ã‚‹éƒ¨åˆ†ãŒã»ã¨ã‚“ã©ã«ãªã£ã¦ã‚‹ã®ã§1ã«ã™ã‚‹\n",
    "predictor.fc1.weight.data[0][0] = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu = float(predictor.state_dict()['fc1.bias'])\n",
    "print(mu_ast)\n",
    "print(hat_mu)\n",
    "hat_sigma = float(predictor.state_dict()['fc1.weight'][0][0])\n",
    "print(sigma_ast)\n",
    "print(hat_sigma)\n",
    "hat_phi = [round(float(predictor.state_dict()['fc1.weight'][0][p-i]),3) for i in range(0, p)]\n",
    "print(phi_ast)\n",
    "print(hat_phi)\n",
    "print(sum([np.abs(phi_ast[i]-hat_phi[i]) for i in range(len(hat_phi))])/len(hat_phi)) # çœŸã®å€¤ã¨ã®çµ¶å¯¾èª¤å·®ã®å¹³å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_floss=np.inf# epochã®flossã®ã®æœ€å°å€¤ã‚’ä¿ç®¡\n",
    "start=time.time()\n",
    "\n",
    "batches_done = 0\n",
    "epoch_done = 0# generatorã‚’å­¦ç¿’ã—ãŸå›æ•°\n",
    "loss_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, opt.n_epochs+1):# epochã”ã¨ã®å‡¦ç†(discriminatorã®epoch)\n",
    "    \n",
    "    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„\n",
    "    loss_list = []\n",
    "    \n",
    "    # batchã®å‡¦ç†ã¯ã€0~892ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆãŸãƒªã‚¹ãƒˆbatch_sampleã‚’ä½œæˆã—ã€ã“ã“ã‹ã‚‰batchÃ—(p+1)ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã¥ã¤ç²å¾—ã™ã‚‹\n",
    "    l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "    batch_sample = random.sample(l, len(l))\n",
    "    for i, batch in enumerate(batch_sample):\n",
    "        \n",
    "        X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "\n",
    "        # generatorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # æ­£è¦åŒ–ã•ã‚ŒãŸinnoationã®æ¨å®šé‡ã‚’generatorã‚’ç”¨ã„ã¦ç®—å‡º\n",
    "        hat_normeps_t = generator(X)\n",
    "        # ã“ã‚Œã¨éå»pæ™‚åˆ»ã®æ™‚ç³»åˆ—ã®å€¤ï¼ˆX_{t-1}, .... , X_{t-p}ï¼‰ã‚’predictorã¸å…¥åŠ›\n",
    "        input_tensor = torch.cat([hat_normeps_t.view(opt.batch_size, -1, 1), X[:,:,:-1]], dim=2)\n",
    "        prediction = predictor(input_tensor)\n",
    "\n",
    "        loss_G = Variable(Wasserstein.pWasserstein(hat_normeps_t.view(opt.batch_size), p=1), requires_grad=True).to(device)\n",
    "        loss_F = mseLoss(prediction, X[:,:,-1])\n",
    "\n",
    "        loss = loss_G+loss_F\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # lossã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³\n",
    "        loss.backward()\n",
    "        # generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã®å¾®åˆ†å€¤ã¨optimizerã‚’ä½¿ã£ã¦æ›´æ–°ã—ã¦ãã‚Œï¼\n",
    "        optimizer_G.step()\n",
    "        optimizer_F.step()\n",
    "\n",
    "        generator_done+=1\n",
    "\n",
    "\n",
    "    print(\"epochï¼š{0}/{1}   batchï¼š{2:003}/{3}   loss_Gï¼š{4: .4f}   loss_Fï¼š{5: .4f}   çµŒéæ™‚é–“ï¼š{6: .1f}ç§’\".format(epoch, opt.n_epochs, i+1, len(batch_sample), round(float(loss_G), 4), round(float(loss_F), 4), time.time()-start))\n",
    "            \n",
    "    if saveModel and epoch % opt.sample_interval == 0:\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„\n",
    "#     try:\n",
    "#         loss_D_curve.append(sum(loss_D_list)/len(loss_D_list))\n",
    "#     except:\n",
    "#         loss_D_curve.append(None)\n",
    "    try:\n",
    "        loss_G_curve.append(sum(loss_G_list)/len(loss_G_list))\n",
    "    except:\n",
    "        loss_G_curve.append(None)\n",
    "    try:\n",
    "        loss_F_curve.append(sum(loss_F_list)/len(loss_F_list))\n",
    "    except:\n",
    "        loss_F_curve.append(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    # validationãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹lossã‚‚è¨ˆç®—ã—ãŸã„\n",
    "    val_hat_normeps_t = generator(valMatrix)\n",
    "    val_normeps_t = torch.randn_like(val_hat_normeps_t)\n",
    "    val_input_tensor = torch.cat([val_hat_normeps_t.view(-1, 1,1), valMatrix[:,:,:-1]], dim=2)\n",
    "    \n",
    "#     val_loss_D = -torch.mean(discriminator(val_normeps_t)) + torch.mean(discriminator(val_hat_normeps_t))\n",
    "#     if opt.withGP:\n",
    "#         val_loss_D = val_loss_D + gradient_penalty(generated_data=val_hat_normeps_t, real_data=val_normeps_t, gp_weight=gp_weight) \n",
    "#     val_loss_D_curve.append(float(val_loss_D))\n",
    "    val_loss_G = Wasserstein.pWasserstein(val_hat_normeps_t.view(-1), p=1)\n",
    "    # if opt.withCorr:\n",
    "    #     val_loss_G = val_loss_G + corr_weight*corr(val_hat_normeps_t)\n",
    "    val_loss_G_curve.append(float(val_loss_G))\n",
    "    \n",
    "    val_loss_F = mseLoss(predictor(val_input_tensor), valMatrix[:,:,0])\n",
    "    val_loss_F_curve.append(float(val_loss_F))\n",
    "\n",
    "    # val_loss_Fã®æœ€å°å€¤ã‚’ä¿ç®¡\n",
    "    if min_floss > val_loss_F_curve[-1]:\n",
    "        min_floss=val_loss_F_curve[-1]\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "        print(\"validationã®flossã®æœ€å°å€¤ã‚’æ›´æ–°ã—ã¾ã—ãŸã€€ã€€Loss:\", min_floss)\n",
    "    \n",
    "    if epoch % 10==0:\n",
    "#         plt.figure(figsize=(13,8))\n",
    "#         plt.title(\"Discriminatorã®Lossã®é·ç§»ã€€\\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.plot(loss_D_curve, label=\"training\")\n",
    "#         plt.plot(val_loss_D_curve, label=\"validation\")\n",
    "#         plt.legend()\n",
    "#         plt.savefig(\"dloss.png\")\n",
    "#         plt.close()\n",
    "\n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"Generatorã®Lossã®é·ç§»ã€€\\nã€€batchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_G_curve, label=\"training\")\n",
    "        plt.plot(val_loss_G_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"gloss.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"Predictorã®Lossã®é·ç§»ã€€\\nã€€batchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_F_curve, label=\"training\")\n",
    "        plt.plot(val_loss_F_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"floss.png\")\n",
    "        plt.close()\n",
    "    \n",
    "torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
