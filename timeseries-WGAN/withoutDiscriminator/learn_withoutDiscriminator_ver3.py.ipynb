{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂÆüÈ®ìNo.0_3\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path=path[:path.find('timeseries-WGAN')+15]\n",
    "# No = (os.path.basename(__file__))[-4]\n",
    "No = \"0_3\" # notebookÁî®\n",
    "print('ÂÆüÈ®ìNo.'+No)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')# Ë≠¶Âëä„ÇíÈùûË°®Á§∫\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(path+\"/\")\n",
    "import random\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import japanize_matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# ‰∫∫Â∑•„Éá„Éº„Çø„ÇíÁîüÊàê„Åó„Å¶„Åè„Çå„ÇãÊ©üÊ¢∞„ÅåÁΩÆ„ÅÑ„Å¶„ÅÇ„Çã„Å®„Åì„Çç\n",
    "import tsModel\n",
    "# Â≠¶ÁøíÁî®„ÅÆ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÅåÁΩÆ„ÅÑ„Å¶„ÅÇ„Çã„Å®„Åì„Çç\n",
    "import models\n",
    "# p-WassersteinË∑ùÈõ¢„ÅÆÈñ¢Êï∞\n",
    "import Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"output-images\"„Éï„Ç©„É´„ÉÄ„Çí‰ΩúÊàêÔºàÊó¢„Å´„ÅÇ„Çã„Å™„Çâ„Åù„Çå„ÅßËâØ„ÅóÔºâ\n",
    "os.makedirs(\"output-images\", exist_ok=True)\n",
    "os.makedirs(\"parameters\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áúü„ÅÆ„É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„Çø\n",
    "phi_ast=[0.3,-0.4,0.2,-0.5,0.6,-0.1,0.1]\n",
    "p_ast=len(phi_ast)\n",
    "mu_ast=0\n",
    "sigma_ast=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ‰ΩúÊàê\n",
    "trainT=1000\n",
    "n=100\n",
    "data_index = range(n)\n",
    "trainDataSets=[]\n",
    "for seed in data_index:\n",
    "    trainData = tsModel.SARIMA(a=phi_ast, N=trainT, random_seed=seed, mu=mu_ast, sigma=sigma_ast)\n",
    "    trainDataSets.append(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, data_seed=0, discriminator_hidden_unit=64, discriminator_seed=0, generator_seed=0, lr=5e-05, n_critic=5, n_epochs=2000, predictor_seed=0, sample_interval=100, training_seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--generator_seed GENERATOR_SEED]\n",
      "                             [--discriminator_seed DISCRIMINATOR_SEED]\n",
      "                             [--predictor_seed PREDICTOR_SEED]\n",
      "                             [--training_seed TRAINING_SEED]\n",
      "                             [--data_seed DATA_SEED] [--n_epochs N_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--n_critic N_CRITIC]\n",
      "                             [--discriminator_hidden_unit DISCRIMINATOR_HIDDEN_UNIT]\n",
      "                             [--sample_interval SAMPLE_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1165/jupyter/kernel-c2ceb9f3-4e7c-4d94-b9c0-366a60a0b39b.json\n"
     ]
    }
   ],
   "source": [
    "# Â≠¶Áøí„Åô„ÇãÊé®ÂÆö„É¢„Éá„É´„ÅÆÂΩ¢Áä∂„ÇÑÂ≠¶ÁøíÊñπÊ≥ï„Å™„Çì„Åã„ÇíÊ±∫ÂÆö„Åó„Åæ„Åô\n",
    "# Â≠¶ÁøíÊôÇ„ÅÆ„Éè„Ç§„Éë„É©„ÅÆÊ±∫ÂÆöÔºàÂÖ•Âäõ„ÇíÂèó„Åë‰ªò„Åë„ÇãÔºâ\n",
    "parser = argparse.ArgumentParser()\n",
    "# „É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ„Å´„Å§„ÅÑ„Å¶\n",
    "parser.add_argument(\"--generator_seed\", type=int, default=0, help=\"generator„ÅÆ„Éë„É©„É°„Éº„Çø„ÅÆÂàùÊúüÂÄ§„ÅÆ„Ç∑„Éº„Éâ\")\n",
    "parser.add_argument(\"--discriminator_seed\", type=int, default=0, help=\"discriminator„ÅÆ„Éë„É©„É°„Éº„Çø„ÅÆÂàùÊúüÂÄ§„ÅÆ„Ç∑„Éº„Éâ\")\n",
    "parser.add_argument(\"--predictor_seed\", type=int, default=0, help=\"predictor„ÅÆ„Éë„É©„É°„Éº„Çø„ÅÆÂàùÊúüÂÄ§„ÅÆ„Ç∑„Éº„Éâ\")\n",
    "parser.add_argument(\"--training_seed\", type=int, default=0, help=\"Ë®ìÁ∑¥„Éá„Éº„Çø„ÇíÂ≠¶Áøí„Åï„Åõ„ÇãÈ†ÜÁï™„ÇíÊ±∫„ÇÅ„Çã„Ç∑„Éº„Éâ\")\n",
    "parser.add_argument(\"--data_seed\", type=int, default=0, help=\"Data„ÅÆ‰ΩúÊàê„Å´Áî®„ÅÑ„Çã‰π±Êï∞„ÅÆseed\")\n",
    "# Â≠¶ÁøíÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=2000, help=\"Discriminator„ÇíÂ≠¶Áøí„Åï„Åõ„ÇãÂõûÊï∞\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batch„ÅÆÂ§ß„Åç„Åï\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"Â≠¶ÁøíÁéá\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"‰∏ÄÂ∫¶generator„ÇíÊõ¥Êñ∞„Åô„Çã„Åî„Å®„Å´‰ΩïÂõûdiscriminator„ÇíÊõ¥Êñ∞„Åô„Çã„Åã\")\n",
    "parser.add_argument(\"--discriminator_hidden_unit\", type=int, default=64, help=\"discriminator„ÅÆÈö†„ÇåÂ±§„ÅÆ„Éã„É•„Éº„É≠„É≥„ÅÆÊï∞\")\n",
    "# parser.add_argument(\"--withGP\", type=bool, default=True, help=\"cliping„ÅÆ‰ª£„Çè„Çä„Å´Gradient Penalty„ÇíÂä†„Åà„Çã„Åã„Å©„ÅÜ„Åã„ÄÇTrue/False\")\n",
    "# parser.add_argument(\"--withCorr\", type=bool, default=True, help=\"Generator„ÅÆÂá∫Âäõ„ÅåbatchÊñπÂêë„Å´ÁÑ°Áõ∏Èñ¢„Å´„Å™„Çã„Çà„ÅÜ„Å™„É≠„Çπ„ÇíÂä†„Åà„Çã„Åã„Å©„ÅÜ„Åã„ÄÇ„ÄÄTrue/False\")\n",
    "# „É¢„Éá„É´„ÅÆ‰øùÂ≠ò„ÇÑLoss„ÅÆÂèØË¶ñÂåñ„Å´„Å§„ÅÑ„Å¶\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"epoch„Çí‰ΩïÂõû„Åæ„Çè„ÅôÂ∫¶„Å´„É¢„Éá„É´„ÅÆ‰øùÂ≠ò„ÇíË°å„ÅÜ„Åã\")\n",
    "\n",
    "try:\n",
    "    opt = parser.parse_args() # .py„ÅÆÂ†¥Âêà„ÅØ„Åì„Å°„Çâ„Çí‰ΩøÁî®(.ipynb„ÅÆÂ†¥Âêà„Ç®„É©„Éº„Å´„Å™„Çä„Åæ„Åô)\n",
    "except:\n",
    "    opt = parser.parse_args(args=[]) # .ipynb„ÅÆÂ†¥Âêà„ÅØ„Åì„Å°„Çâ„Çí‰ΩøÁî®\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU„Åå‰Ωø„Åà„Åæ„Åô„ÄÇ\n",
      "GPU„Çí‰Ωø„ÅÑ„Åæ„Åô„ÅãÔºü ÔºàYesÔºö1, NoÔºö0Ôºâ  ----> 1\n",
      "‰ΩøÁî®„Åô„ÇãGPU„ÅÆÁï™Âè∑„ÇíÂÖ•„Çå„Å¶„Åè„Å†„Åï„ÅÑ : 0\n"
     ]
    }
   ],
   "source": [
    "# gpu„Åå‰Ωø„Åà„Çã„Åã„Å©„ÅÜ„Åã\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    print(\"GPU„Åå‰Ωø„Åà„Åæ„Åô„ÄÇ\")\n",
    "    use_gpu = input('GPU„Çí‰Ωø„ÅÑ„Åæ„Åô„ÅãÔºü ÔºàYesÔºö1, NoÔºö0Ôºâ  ----> ')\n",
    "    cuda = bool(int(use_gpu))\n",
    "else:\n",
    "    print(\"GPU„ÅØ‰Ωø„Åà„Åæ„Åõ„Çì„ÄÇ\")\n",
    "if cuda:\n",
    "    gpu_id = input('‰ΩøÁî®„Åô„ÇãGPU„ÅÆÁï™Âè∑„ÇíÂÖ•„Çå„Å¶„Åè„Å†„Åï„ÅÑ : ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "device = torch.device('cuda:'+gpu_id if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êé®ÂÆö„É¢„Éá„É´„ÅÆÊ±∫ÂÆö\n",
    "p=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output-images/p{0}\".format(p), exist_ok=True)\n",
    "os.makedirs(\"parameters/p{0}\".format(p), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt.generator_seed)\n",
    "generator = models.LinearGenerator(p = p, input_dim=1, is_bias=False)\n",
    "torch.manual_seed(opt.predictor_seed)\n",
    "predictor = models.LinearPredictNet(p=p, input_dim=1, is_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë®ìÁ∑¥„Éá„Éº„Çø„Çí‰∏Ä„Å§Áî®„ÅÑ„Å¶Â≠¶Áøí„Åï„Åõ„Çã\n",
    "dataSeed=opt.data_seed\n",
    "# „Åì„ÅÑ„Å§„Çítrain:validation=900:100„Å´ÂàÜÂâ≤„Åô„Çã\n",
    "Data = trainDataSets[dataSeed]\n",
    "Data = torch.tensor(Data, dtype=torch.float)\n",
    "Data=Data.view(1,-1)\n",
    "trainData = Data[:,:900]\n",
    "valData = Data[:,900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData„Å®valData„Çí {ùëãùë°}ùë°0ùë°=ùë°0‚àíùëù „Åî„Å®„Å´Âèñ„ÇäÂá∫„Åó„ÇÑ„Åô„ÅÑ„Çà„ÅÜ„Å´Matrix„Å´Â§âÊèõ„Åô„Çã\n",
    "trainMatrix = []\n",
    "for i in range(trainData.shape[1]-(p)):\n",
    "    ans = trainData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    trainMatrix.append(ans)\n",
    "trainMatrix = torch.cat(trainMatrix)\n",
    "valMatrix = []\n",
    "for i in range(valData.shape[1]-(p)):\n",
    "    ans = valData[:,i:i+p+1].view(1,Data.shape[0],-1)\n",
    "    valMatrix.append(ans)\n",
    "valMatrix = torch.cat(valMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Çí 100 epoch„Åî„Å®„Å´ÈÄêÊ¨°‰øùÂ≠ò„Åó„Åæ„Åô„ÅãÔºü ÔºàYesÔºö1, NoÔºö0Ôºâ  ----> 1\n"
     ]
    }
   ],
   "source": [
    "# Optimizers(„Éë„É©„É°„Éº„Çø„Å´ÂØæ„Åó„Å¶ÂÆöÁæ©„Åï„Çå„Çã)\n",
    "optimizer_G = torch.optim.RMSprop(params=generator.parameters(), lr=opt.lr)\n",
    "# optimizer_D = torch.optim.RMSprop(params=discriminator.parameters(), lr=opt.lr)\n",
    "optimizer_F = torch.optim.RMSprop(params=predictor.parameters(), lr=opt.lr)\n",
    "# optimizer_F = torch.optim.Adam(params=predictor.parameters())\n",
    "\n",
    "# ‰∫åÊù°Ë™§Â∑ÆMSE\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "# „Éë„É©„É°„Éº„Çø„Å®Â≠¶Áøí„Éá„Éº„Çø„ÇíGPU„Å´‰πó„Å£„Åë„Çã\n",
    "generator.to(device)\n",
    "# discriminator.to(device)\n",
    "predictor.to(device)\n",
    "trainMatrix=trainMatrix.to(device)\n",
    "valMatrix=valMatrix.to(device)\n",
    "mseLoss.to(device)\n",
    "\n",
    "saveModel = input('‰ΩúÊàê„Åó„Åü„É¢„Éá„É´„Çí {0} epoch„Åî„Å®„Å´ÈÄêÊ¨°‰øùÂ≠ò„Åó„Åæ„Åô„ÅãÔºü ÔºàYesÔºö1, NoÔºö0Ôºâ  ----> '.format(opt.sample_interval))\n",
    "saveModel = bool(int(saveModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰∫ãÂâçÂ≠¶Áøí„Çí„Åì„Åì„ÅßË°å„ÅÑ„Åæ„Åô„Åã„ÄÅ„Åù„Çå„Å®„ÇÇË™≠„ÅøËæº„Åø„Åæ„Åô„Åã ÔºàË°å„ÅÜÔºö1, Ë™≠„ÅøËæº„ÇÄÔºö0Ôºâ  ----> 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c6a1a62dad8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdo_preTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'‰∫ãÂâçÂ≠¶Áøí„Çí„Åì„Åì„ÅßË°å„ÅÑ„Åæ„Åô„Åã„ÄÅ„Åù„Çå„Å®„ÇÇË™≠„ÅøËæº„Åø„Åæ„Åô„Åã ÔºàË°å„ÅÜÔºö1, Ë™≠„ÅøËæº„ÇÄÔºö0Ôºâ  ----> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpretrain_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'parameters/p{0}/No{1}_predictor_epoch{2}_batchSize{3}_DataSeed{4}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSeed\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_preTrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# „É¢„Éá„É´„Éë„É©„É°„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÇ„ÅÜ„Å®„Åó„Å¶Â§±Êïó„Åó„Åü„Çâ„ÄÅ„Åù„Çå„ÅØ„Éï„Ç°„Ç§„É´„Åå„Å™„ÅÑ„Å®Ë®Ä„ÅÜ„Åì„Å®„Å™„ÅÆ„Åß„ÄÅ‰∫ãÂâçÂ≠¶Áøí„Çí„Åì„ÅÆÂ†¥„ÅßË°å„ÅÜ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "# Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÊôÇÁ≥ªÂàó„ÅÆ„Å©„ÅÆÊôÇÂàª„ÇíÂ≠¶Áøí„Å´Áî®„ÅÑ„Çã„Åã„Çí„É©„É≥„ÉÄ„É†„Å´„Åó„Å¶„ÅÑ„Çã„Åå„ÄÅ„Åù„ÅÆ„É©„É≥„ÉÄ„É†„Ç∑„Éº„Éâ„ÇíÂõ∫ÂÆö„Åô„Çã\n",
    "random.seed(a=opt.training_seed)\n",
    "\n",
    "do_preTrain = bool(int(input('‰∫ãÂâçÂ≠¶Áøí„Çí„Åì„Åì„ÅßË°å„ÅÑ„Åæ„Åô„Åã„ÄÅ„Åù„Çå„Å®„ÇÇË™≠„ÅøËæº„Åø„Åæ„Åô„Åã ÔºàË°å„ÅÜÔºö1, Ë™≠„ÅøËæº„ÇÄÔºö0Ôºâ  ----> ')))\n",
    "pretrain_param = 'parameters/p{0}/No{1}_predictor_epoch{2}_batchSize{3}_DataSeed{4}.pth'.format(p, No, 0, opt.batch_size, dataSeed )\n",
    "if not do_preTrain:\n",
    "    try:# „É¢„Éá„É´„Éë„É©„É°„Éº„Çø„ÇíË™≠„ÅøËæº„ÇÇ„ÅÜ„Å®„Åó„Å¶Â§±Êïó„Åó„Åü„Çâ„ÄÅ„Åù„Çå„ÅØ„Éï„Ç°„Ç§„É´„Åå„Å™„ÅÑ„Å®Ë®Ä„ÅÜ„Åì„Å®„Å™„ÅÆ„Åß„ÄÅ‰∫ãÂâçÂ≠¶Áøí„Çí„Åì„ÅÆÂ†¥„ÅßË°å„ÅÜ\n",
    "        predictor.load_state_dict(torch.load(pretrain_param)) \n",
    "    except:\n",
    "        print(\"„É¢„Éá„É´„ÅåÂ≠òÂú®„Åó„Å™„ÅÑ„ÅÆ„Åß‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„ÅÑ„Åæ„Åô\")\n",
    "        do_preTrain=True\n",
    "\n",
    "if do_preTrain:\n",
    "    # „Åì„Åì„Åß„Åæ„Åö„ÅØF„ÅÆ‰∫ãÂâçÂ≠¶Áøí„ÇíË°å„ÅÜ\n",
    "    loss_pre = []\n",
    "    val_loss_pre = []\n",
    "    pretrain_epoch = 1000\n",
    "    start=time.time()\n",
    "    for epoch in range(1, pretrain_epoch+1):# epoch„Åî„Å®„ÅÆÂá¶ÁêÜ\n",
    "        # batch„ÅÆÂá¶ÁêÜ„ÅØ„ÄÅ0~892„Çí„É©„É≥„ÉÄ„É†„Å´‰∏¶„Å≥Êõø„Åà„Åü„É™„Çπ„Éàbatch_sample„Çí‰ΩúÊàê„Åó„ÄÅ„Åì„Åì„Åã„Çâbatch√ó(p+1)„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„Çí‰∏Ä„Å§„Å•„Å§Áç≤Âæó„Åô„Çã\n",
    "        l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "        batch_sample = random.sample(l, len(l))\n",
    "        for i, batch in enumerate(batch_sample):\n",
    "            X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "            optimizer_F.zero_grad()\n",
    "            input_tensor = X[:,:,:-1]\n",
    "            input_tensor = torch.cat([torch.randn([opt.batch_size,1,1]).to(device), input_tensor], dim=2)\n",
    "            true_tensor = X[:,:,-1]\n",
    "            prediction = predictor(input_tensor)\n",
    "            loss_F = mseLoss(prediction, true_tensor)\n",
    "            loss_F.backward()\n",
    "            optimizer_F.step()\n",
    "        loss_pre.append(loss_F.item())\n",
    "        \n",
    "        val_input = torch.cat([torch.randn([valMatrix.shape[0],1,1]).to(device), valMatrix[:,:,:-1]], dim=2)\n",
    "        val_target = valMatrix[:,:,-1].view(valMatrix.shape[0], -1)\n",
    "        val_loss = mseLoss(predictor(val_input), val_target)\n",
    "        val_loss_pre.append(val_loss.item())\n",
    "        \n",
    "        print(\"epochÔºö{0}/{1}   loss_FÔºö{2: .4f}   ÁµåÈÅéÊôÇÈñìÔºö{3: .1f}Áßí\".format(epoch, pretrain_epoch, round(loss_F.item(), 4), time.time()-start))\n",
    "        if epoch % 100==0:\n",
    "            plt.figure(figsize=(13,8))\n",
    "            plt.title(\"Predictor„ÅÆLoss„ÅÆÈÅ∑Áßª„ÄÄ\\n„ÄÄbatchSize:{0}\".format(opt.batch_size))\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.plot(loss_pre, label=\"training\")\n",
    "            plt.plot(val_loss_pre, label=\"validation\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"preloss.png\")\n",
    "            plt.close()\n",
    "    torch.save(predictor.state_dict(), pretrain_param)\n",
    "    \n",
    "    plt.figure(figsize=(13,8))\n",
    "    plt.title(\"Predictor„ÅÆLoss„ÅÆÈÅ∑Áßª„ÄÄ\\n„ÄÄbatchSize:{0}\".format(opt.batch_size))\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_pre, label=\"training\")\n",
    "    plt.plot(val_loss_pre, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"output-images/p{0}/No{1}_preloss_epoch{2}_batchSize{3}_DataSeed{4}.png\".format(p, No, epoch, opt.batch_size, dataSeed ))\n",
    "    plt.close()\n",
    "    print(\"pre-trainingÁµÇ‰∫Ü\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hat_sigma„Å´Áõ∏ÂΩì„Åô„ÇãÈÉ®ÂàÜ„Åå„Åª„Å®„Çì„Å©„Å´„Å™„Å£„Å¶„Çã„ÅÆ„Åß1„Å´„Åô„Çã\n",
    "predictor.fc1.weight.data[0][0] = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.05302155762910843\n",
      "2\n",
      "1.0\n",
      "[0.3, -0.4, 0.2, -0.5, 0.6, -0.1, 0.1]\n",
      "[0.259, -0.381, 0.219, -0.523, 0.56, -0.056, 0.127]\n",
      "0.03042857142857142\n"
     ]
    }
   ],
   "source": [
    "hat_mu = float(predictor.state_dict()['fc1.bias'])\n",
    "print(mu_ast)\n",
    "print(hat_mu)\n",
    "hat_sigma = float(predictor.state_dict()['fc1.weight'][0][0])\n",
    "print(sigma_ast)\n",
    "print(hat_sigma)\n",
    "hat_phi = [round(float(predictor.state_dict()['fc1.weight'][0][p-i]),3) for i in range(0, p)]\n",
    "print(phi_ast)\n",
    "print(hat_phi)\n",
    "print(sum([np.abs(phi_ast[i]-hat_phi[i]) for i in range(len(hat_phi))])/len(hat_phi)) # Áúü„ÅÆÂÄ§„Å®„ÅÆÁµ∂ÂØæË™§Â∑Æ„ÅÆÂπ≥Âùá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_floss=np.inf# epoch„ÅÆfloss„ÅÆ„ÅÆÊúÄÂ∞èÂÄ§„Çí‰øùÁÆ°\n",
    "start=time.time()\n",
    "\n",
    "batches_done = 0\n",
    "epoch_done = 0# generator„ÇíÂ≠¶Áøí„Åó„ÅüÂõûÊï∞\n",
    "loss_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, opt.n_epochs+1):# epoch„Åî„Å®„ÅÆÂá¶ÁêÜ(discriminator„ÅÆepoch)\n",
    "    \n",
    "    # epoch„Åî„Å®„Å´batch„ÅßË®àÁÆó„Åó„Åüloss„ÇíÂπ≥Âùá„Åó„ÅüÂÄ§„Çíloss_curve„Å®„Åó„Å¶Êèè„Åç„Åü„ÅÑ\n",
    "    loss_list = []\n",
    "    \n",
    "    # batch„ÅÆÂá¶ÁêÜ„ÅØ„ÄÅ0~892„Çí„É©„É≥„ÉÄ„É†„Å´‰∏¶„Å≥Êõø„Åà„Åü„É™„Çπ„Éàbatch_sample„Çí‰ΩúÊàê„Åó„ÄÅ„Åì„Åì„Åã„Çâbatch√ó(p+1)„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„Çí‰∏Ä„Å§„Å•„Å§Áç≤Âæó„Åô„Çã\n",
    "    l=list(range(trainMatrix.shape[0]-opt.batch_size))\n",
    "    batch_sample = random.sample(l, len(l))\n",
    "    for i, batch in enumerate(batch_sample):\n",
    "        \n",
    "        X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)\n",
    "\n",
    "        # generator„ÅÆÂãæÈÖçÊÉÖÂ†±„Çí0„Å´ÂàùÊúüÂåñ\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Ê≠£Ë¶èÂåñ„Åï„Çå„Åüinnoation„ÅÆÊé®ÂÆöÈáè„Çígenerator„ÇíÁî®„ÅÑ„Å¶ÁÆóÂá∫\n",
    "        hat_normeps_t = generator(X)\n",
    "        # „Åì„Çå„Å®ÈÅéÂéªpÊôÇÂàª„ÅÆÊôÇÁ≥ªÂàó„ÅÆÂÄ§ÔºàX_{t-1}, .... , X_{t-p}Ôºâ„Çípredictor„Å∏ÂÖ•Âäõ\n",
    "        input_tensor = torch.cat([hat_normeps_t.view(opt.batch_size, -1, 1), X[:,:,:-1]], dim=2)\n",
    "        prediction = predictor(input_tensor)\n",
    "\n",
    "        loss_G = Variable(Wasserstein.pWasserstein(hat_normeps_t.view(opt.batch_size), p=1), requires_grad=True).to(device)\n",
    "        loss_F = mseLoss(prediction, X[:,:,-1])\n",
    "\n",
    "        loss = loss_G+loss_F\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # loss„ÇíÁõÆÁöÑÈñ¢Êï∞„Å®„Åó„Å¶„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂÖ®„Éë„É©„É°„Éº„Çø„ÅßÂæÆÂàÜ„Çí„Åó„Å¶„Åè„Çå„Å®Ë®Ä„ÅÜÂêàÂõ≥\n",
    "        loss.backward()\n",
    "        # generator„ÅÆ„Éë„É©„É°„Éº„Çø„Çí„Åù„ÅÆÂæÆÂàÜÂÄ§„Å®optimizer„Çí‰Ωø„Å£„Å¶Êõ¥Êñ∞„Åó„Å¶„Åè„ÇåÔºÅ\n",
    "        optimizer_G.step()\n",
    "        optimizer_F.step()\n",
    "\n",
    "        generator_done+=1\n",
    "\n",
    "\n",
    "    print(\"epochÔºö{0}/{1}   batchÔºö{2:003}/{3}   loss_GÔºö{4: .4f}   loss_FÔºö{5: .4f}   ÁµåÈÅéÊôÇÈñìÔºö{6: .1f}Áßí\".format(epoch, opt.n_epochs, i+1, len(batch_sample), round(float(loss_G), 4), round(float(loss_F), 4), time.time()-start))\n",
    "            \n",
    "    if saveModel and epoch % opt.sample_interval == 0:\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "    # epoch„Åî„Å®„Å´batch„ÅßË®àÁÆó„Åó„Åüloss„ÇíÂπ≥Âùá„Åó„ÅüÂÄ§„Çíloss_curve„Å®„Åó„Å¶Êèè„Åç„Åü„ÅÑ\n",
    "#     try:\n",
    "#         loss_D_curve.append(sum(loss_D_list)/len(loss_D_list))\n",
    "#     except:\n",
    "#         loss_D_curve.append(None)\n",
    "    try:\n",
    "        loss_G_curve.append(sum(loss_G_list)/len(loss_G_list))\n",
    "    except:\n",
    "        loss_G_curve.append(None)\n",
    "    try:\n",
    "        loss_F_curve.append(sum(loss_F_list)/len(loss_F_list))\n",
    "    except:\n",
    "        loss_F_curve.append(None)\n",
    "    \n",
    "\n",
    "    \n",
    "    # validation„Éá„Éº„Çø„Å´„Çà„Çãloss„ÇÇË®àÁÆó„Åó„Åü„ÅÑ\n",
    "    val_hat_normeps_t = generator(valMatrix)\n",
    "    val_normeps_t = torch.randn_like(val_hat_normeps_t)\n",
    "    val_input_tensor = torch.cat([val_hat_normeps_t.view(-1, 1,1), valMatrix[:,:,:-1]], dim=2)\n",
    "    \n",
    "#     val_loss_D = -torch.mean(discriminator(val_normeps_t)) + torch.mean(discriminator(val_hat_normeps_t))\n",
    "#     if opt.withGP:\n",
    "#         val_loss_D = val_loss_D + gradient_penalty(generated_data=val_hat_normeps_t, real_data=val_normeps_t, gp_weight=gp_weight) \n",
    "#     val_loss_D_curve.append(float(val_loss_D))\n",
    "    val_loss_G = Wasserstein.pWasserstein(val_hat_normeps_t.view(-1), p=1)\n",
    "    # if opt.withCorr:\n",
    "    #     val_loss_G = val_loss_G + corr_weight*corr(val_hat_normeps_t)\n",
    "    val_loss_G_curve.append(float(val_loss_G))\n",
    "    \n",
    "    val_loss_F = mseLoss(predictor(val_input_tensor), valMatrix[:,:,0])\n",
    "    val_loss_F_curve.append(float(val_loss_F))\n",
    "\n",
    "    # val_loss_F„ÅÆÊúÄÂ∞èÂÄ§„Çí‰øùÁÆ°\n",
    "    if min_floss > val_loss_F_curve[-1]:\n",
    "        min_floss=val_loss_F_curve[-1]\n",
    "        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_minLoss_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "\n",
    "        print(\"validation„ÅÆfloss„ÅÆÊúÄÂ∞èÂÄ§„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü„ÄÄ„ÄÄLoss:\", min_floss)\n",
    "    \n",
    "    if epoch % 10==0:\n",
    "#         plt.figure(figsize=(13,8))\n",
    "#         plt.title(\"Discriminator„ÅÆLoss„ÅÆÈÅ∑Áßª„ÄÄ\\n„ÄÄbatchSize:{0}, GP„ÅÆ‰øÇÊï∞:{1}, Corr„ÅÆ‰øÇÊï∞:{2}\".format(opt.batch_size, gp_weight, corr_weight))\n",
    "#         plt.xlabel(\"epoch\")\n",
    "#         plt.ylabel(\"Loss\")\n",
    "#         plt.plot(loss_D_curve, label=\"training\")\n",
    "#         plt.plot(val_loss_D_curve, label=\"validation\")\n",
    "#         plt.legend()\n",
    "#         plt.savefig(\"dloss.png\")\n",
    "#         plt.close()\n",
    "\n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"Generator„ÅÆLoss„ÅÆÈÅ∑Áßª„ÄÄ\\n„ÄÄbatchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_G_curve, label=\"training\")\n",
    "        plt.plot(val_loss_G_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"gloss.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"Predictor„ÅÆLoss„ÅÆÈÅ∑Áßª„ÄÄ\\n„ÄÄbatchSize:{0}\".format(opt.batch_size))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_F_curve, label=\"training\")\n",
    "        plt.plot(val_loss_F_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"floss.png\")\n",
    "        plt.close()\n",
    "    \n",
    "torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/No{0}_generator_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n",
    "torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/No{0}_predictor_epoch{1}_batchSize{2}_DataSeed{3}.pth'.format(No, epoch, opt.batch_size, dataSeed))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
