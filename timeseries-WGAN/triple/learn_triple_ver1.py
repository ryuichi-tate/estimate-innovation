
'''
triple networkã®å­¦ç¿’ï¼ˆå‘½åãƒ˜ã‚¿ã‹(ç¬‘)ï¼‰
'''
# %matplotlib inline
import argparse
import os
path = os.getcwd()
path=path[:path.find('timeseries-WGAN')+15]
import warnings
warnings.simplefilter('ignore')# è­¦å‘Šã‚’éè¡¨ç¤º
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import math
import sys
sys.path.append(path+"/")
import random
import time
import statsmodels.api as sm
from scipy.stats import norm
import japanize_matplotlib
from scipy.stats import gaussian_kde

import torchvision.transforms as transforms
from torchvision.utils import save_image

import torch.nn as nn
import torch.nn.functional as F
import torch
from torch.autograd import Variable
# äººå·¥ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹æ©Ÿæ¢°ãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import tsModel
# å­¦ç¿’ç”¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒç½®ã„ã¦ã‚ã‚‹ã¨ã“ã‚
import models

# "output-images"ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«ã‚ã‚‹ãªã‚‰ãã‚Œã§è‰¯ã—ï¼‰
os.makedirs("output-images", exist_ok=True)
os.makedirs("parameters", exist_ok=True)

# çœŸã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
phi_ast=[0.3,-0.4,0.2,-0.5,0.6,-0.1,0.1]
p_ast=len(phi_ast)
mu_ast=0
sigma_ast=2

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
trainT=1000
n=100
data_index = range(n)
trainDataSets=[]
for seed in data_index:
    trainData = tsModel.SARIMA(a=phi_ast, N=trainT, random_seed=seed, mu=mu_ast, sigma=sigma_ast)
    trainDataSets.append(trainData)


# å­¦ç¿’ã™ã‚‹æ¨å®šãƒ¢ãƒ‡ãƒ«ã®å½¢çŠ¶ã‚„å­¦ç¿’æ–¹æ³•ãªã‚“ã‹ã‚’æ±ºå®šã—ã¾ã™
# å­¦ç¿’æ™‚ã®ãƒã‚¤ãƒ‘ãƒ©ã®æ±ºå®šï¼ˆå…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ï¼‰
parser = argparse.ArgumentParser()
# ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã¤ã„ã¦
parser.add_argument("--generator_seed", type=int, default=0, help="generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--discriminator_seed", type=int, default=0, help="discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--predictor_seed", type=int, default=0, help="predictorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã®ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--training_seed", type=int, default=0, help="è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã•ã›ã‚‹é †ç•ªã‚’æ±ºã‚ã‚‹ã‚·ãƒ¼ãƒ‰")
parser.add_argument("--data_seed", type=int, default=0, help="Dataã®ä½œæˆã«ç”¨ã„ã‚‹ä¹±æ•°ã®seed")
# å­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦
parser.add_argument("--n_epochs", type=int, default=2000, help="Discriminatorã‚’å­¦ç¿’ã•ã›ã‚‹å›æ•°")
parser.add_argument("--batch_size", type=int, default=64, help="batchã®å¤§ãã•")
parser.add_argument("--lr", type=float, default=0.00005, help="å­¦ç¿’ç‡")
parser.add_argument("--n_critic", type=int, default=5, help="ä¸€åº¦generatorã‚’æ›´æ–°ã™ã‚‹ã”ã¨ã«ä½•å›discriminatorã‚’æ›´æ–°ã™ã‚‹ã‹")
parser.add_argument("--discriminator_hidden_unit", type=int, default=64, help="discriminatorã®éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•°")
parser.add_argument("--withGP", type=bool, default=True, help="clipingã®ä»£ã‚ã‚Šã«Gradient Penaltyã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚True/False")
parser.add_argument("--withCorr", type=bool, default=True, help="Generatorã®å‡ºåŠ›ãŒbatchæ–¹å‘ã«ç„¡ç›¸é–¢ã«ãªã‚‹ã‚ˆã†ãªãƒ­ã‚¹ã‚’åŠ ãˆã‚‹ã‹ã©ã†ã‹ã€‚ã€€True/False")
# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚„Lossã®å¯è¦–åŒ–ã«ã¤ã„ã¦
parser.add_argument("--sample_interval", type=int, default=1000, help="epochã‚’ä½•å›ã¾ã‚ã™åº¦ã«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã‚’è¡Œã†ã‹")

try:
    opt = parser.parse_args() # .pyã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨(.ipynbã®å ´åˆã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™)
except:
    opt = parser.parse_args(args=[]) # .ipynbã®å ´åˆã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨
print(opt)

# Gradient Penaltyã‚’Lossã«åŠ ãˆã‚‹éš›ã®é‡ã¿ã®è¨­å®š
if opt.withGP:
    default_weight=1.0
    gp_weight = input("Gradient Penaltyã‚’Lossã«åŠ ãˆã‚‹æ™‚ã®é‡ã¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(defaultã¯{0})ï¼š".format(default_weight))
    try:
        gp_weight = float(gp_weight)
        if gp_weight<0:
            gp_weight = default_weight
    except:
        gp_weight=default_weight
    print("Gradient Penaltyã®Losså†…ã§ã®é‡ã¿ã¯{0}ã§ã™ã€‚".format(gp_weight))
else:
    clip_value = input('discriminatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’clipã™ã‚‹å€¤(æ­£ã®æ•°)ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„(defaultã¯0.01)ï¼š')
    try:
        clip_value=float(clip_value)
        if clip_value<=0:
            clip_value=0.01
    except:
        clip_value=0.01
    print("clipå€¤ã¯",clip_value,"ã§ã™ã€‚")
    gp_weight=0
    
# ç›¸é–¢ä¿‚æ•°ã®åˆ¶ç´„ã‚’Lossã«åŠ ãˆã‚‹éš›ã®é‡ã¿ã®è¨­å®š
default_weight = 1.0
if opt.withCorr:
    corr_weight= input("ç›¸é–¢ä¿‚æ•°ã®åˆ¶ç´„ã‚’Lossã«åŠ ãˆã‚‹æ™‚ã®é‡ã¿ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(defaultã¯{0})ï¼š".format(default_weight))
    try:
        corr_weight = float(corr_weight)
        if corr_weight<0:
            corr_weight = default_weight
    except:
        corr_weight=default_weight
    print("ç›¸é–¢ä¿‚æ•°ã«é–¢ã™ã‚‹åˆ¶ç´„ã®Losså†…ã§ã®é‡ã¿ã¯{0}ã§ã™ã€‚".format(corr_weight))
else:
    corr_weight=0

# Gradient Penaltyé …ã®è¨ˆç®—
def gradient_penalty(generated_data, real_data, gp_weight=10):
    batch_size = real_data.size()[0]

    alpha = torch.rand(batch_size, 1)
    alpha = alpha.expand_as(real_data)
    if cuda:
        alpha=alpha.to(device)

    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data
    interpolated = Variable(interpolated, requires_grad=True)
    if cuda:
        interpolated=interpolated.to(device)

    # Calculate probability of interpolated examples
    prob_interpolated = discriminator(interpolated)

    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,
                               grad_outputs=torch.ones(prob_interpolated.size()).to(device) if cuda else torch.ones(
                               prob_interpolated.size()),
                               create_graph=True, retain_graph=True)[0]

    gradients = gradients.view(batch_size, -1)# ã“ã‚Œã„ã‚‰ãªã„ã‹ã‚‚...
    
    # gradients_norm = (gradients.norm(2, dim=1) - 1) ** 2
    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)# 0é™¤ç®—ã‚’é˜²ãï¼Ÿ

    return gp_weight * ((gradients_norm - 1) ** 2).mean()

def corr(x):
    return (((x-x.mean())*(x-x.mean()).T)*(1-torch.eye(x.shape[0],x.shape[0]).to(device))).sum()/2/x.shape[0]

# gpuãŒä½¿ãˆã‚‹ã‹ã©ã†ã‹
cuda = True if torch.cuda.is_available() else False
if cuda:
    print("GPUãŒä½¿ãˆã¾ã™ã€‚")
    use_gpu = input('GPUã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> ')
    cuda = bool(int(use_gpu))
else:
    print("GPUã¯ä½¿ãˆã¾ã›ã‚“ã€‚")
if cuda:
    gpu_id = input('ä½¿ç”¨ã™ã‚‹GPUã®ç•ªå·ã‚’å…¥ã‚Œã¦ãã ã•ã„ : ')
    os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
device = torch.device('cuda:'+gpu_id if cuda else 'cpu')


# æ¨å®šãƒ¢ãƒ‡ãƒ«ã®æ±ºå®š
p=7

os.makedirs("output-images/p{0}".format(p), exist_ok=True)
os.makedirs("parameters/p{0}".format(p), exist_ok=True)

torch.manual_seed(opt.generator_seed)
generator = models.LinearGenerator(p = p, input_dim=1, is_bias=False)
torch.manual_seed(opt.discriminator_seed)
discriminator = models.Discriminator(q=0, discriminator_hidden_unit=opt.discriminator_hidden_unit)
torch.manual_seed(opt.predictor_seed)
predictor = models.LinearPredictNet(p=p, input_dim=1, is_bias=True)


# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ç”¨ã„ã¦å­¦ç¿’ã•ã›ã‚‹
dataSeed=opt.data_seed
# ã“ã„ã¤ã‚’train:validation=900:100ã«åˆ†å‰²ã™ã‚‹
Data = trainDataSets[dataSeed]
Data = torch.tensor(Data, dtype=torch.float)
Data=Data.view(1,-1)
trainData = Data[:,:900]
valData = Data[:,900:]
# trainDataã¨valDataã‚’ {ğ‘‹ğ‘¡}ğ‘¡0ğ‘¡=ğ‘¡0âˆ’ğ‘ ã”ã¨ã«å–ã‚Šå‡ºã—ã‚„ã™ã„ã‚ˆã†ã«Matrixã«å¤‰æ›ã™ã‚‹
trainMatrix = []
for i in range(trainData.shape[1]-(p)):
    ans = trainData[:,i:i+p+1].view(1,Data.shape[0],-1)
    trainMatrix.append(ans)
trainMatrix = torch.cat(trainMatrix)
valMatrix = []
for i in range(valData.shape[1]-(p)):
    ans = valData[:,i:i+p+1].view(1,Data.shape[0],-1)
    valMatrix.append(ans)
valMatrix = torch.cat(valMatrix)

# å­¦ç¿’

# Optimizers(ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹)
optimizer_G = torch.optim.RMSprop(params=generator.parameters(), lr=opt.lr)
optimizer_D = torch.optim.RMSprop(params=discriminator.parameters(), lr=opt.lr)
optimizer_F = torch.optim.Adam(params=predictor.parameters())
# äºŒæ¡èª¤å·®MSE
mseLoss = nn.MSELoss()
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ä¹—ã£ã‘ã‚‹
generator.to(device)
discriminator.to(device)
predictor.to(device)

mseLoss.to(device)

trainMatrix=trainMatrix.to(device)
valMatrix=valMatrix.to(device)

saveModel = input('ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’{0}epochã”ã¨ã«é€æ¬¡ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ ï¼ˆYesï¼š1, Noï¼š0ï¼‰  ----> '.format(opt.sample_interval))
saveModel = bool(int(saveModel))

batches_done = 0
generator_done = 0# generatorã‚’å­¦ç¿’ã—ãŸå›æ•°
predictor_done = 0

# ã‚°ãƒ©ãƒ•æç”»ç”¨
loss_D_curve = []
loss_G_curve = []
loss_F_curve = []
val_loss_D_curve = []
val_loss_G_curve = []
val_loss_F_curve = []
p_value = []

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ã®ã©ã®æ™‚åˆ»ã‚’å­¦ç¿’ã«ç”¨ã„ã‚‹ã‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ã„ã‚‹ãŒã€ãã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã™ã‚‹
random.seed(a=opt.training_seed)

start=time.time()
for epoch in range(1, opt.n_epochs+1):# epochã”ã¨ã®å‡¦ç†(discriminatorã®epoch)
    
    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„
    loss_D_list = []
    loss_G_list = []
    loss_F_list = []
    
    # batchã®å‡¦ç†ã¯ã€0~892ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸¦ã³æ›¿ãˆãŸãƒªã‚¹ãƒˆbatch_sampleã‚’ä½œæˆã—ã€ã“ã“ã‹ã‚‰batchÃ—(p+1)ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã¥ã¤ç²å¾—ã™ã‚‹
    l=list(range(trainMatrix.shape[0]-opt.batch_size))
    batch_sample = random.sample(l, len(l))
    for i, batch in enumerate(batch_sample):
        
        X = trainMatrix[batch : batch+opt.batch_size]# torch.Size([64, 1, 8]) (batch, dim, p+1)
        X = Variable(X)# è‡ªå‹•å¾®åˆ†å¯èƒ½ã«ã€‚(ã‚‚ã†ã„ã‚‰ãªã„ã‚‰ã—ã„ï¼Ÿ)
        
        
        # ---------------------
        #  Train Discriminator
        # ---------------------
        
        for param in discriminator.parameters(): # discriminatorã®å‹¾é…ã®æ›´æ–°ã‚’ONã«ã™ã‚‹
            param.requires_grad = True 
            
        if not opt.withGP:
            # discriminatorã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ï¼ˆå…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã®çµ¶å¯¾å€¤ãŒclip_valueä»¥ä¸‹ã®å€¤ã«ãªã‚‹ï¼‰
            for param in discriminator.parameters():
                param.data.clamp_(-clip_value, clip_value)

        # discriminatorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–ã™ã‚‹
        optimizer_D.zero_grad()
        
        # ç¾åœ¨ï¼‹éå»pæ™‚åˆ»åˆ†ã®æ™‚ç³»åˆ—ã‚’generatorã§å¤‰æ›ã—ãŸå€¤ã‚’å–å¾—
        hat_normeps_t = generator(X)#.detach() # torch.Size([64, 4])
        
        # generatorã®å‡ºåŠ›ã¨åŒã˜å¤§ãã•ã®æ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
        normeps_t = Variable(torch.randn_like(hat_normeps_t))
        
        # Adversarial loss ã™ãªã‚ã¡Wassersteinè·é›¢ã®ç¬¦å·ã‚’åè»¢ã•ã›ãŸã‚‚ã®ã€‚ï¼ˆDiscriminatorã¯Wassersteinè·é›¢ã‚’æœ€å¤§ã«ã™ã‚‹é–¢æ•°ã«ãªã‚ŠãŸã„ï¼‰
        loss_D = -torch.mean(discriminator(normeps_t)) + torch.mean(discriminator(hat_normeps_t))
        # GPã‚‚åŠ ãˆã¨ã
        if opt.withGP:
            loss_D = loss_D + gradient_penalty(generated_data=hat_normeps_t, real_data=normeps_t, gp_weight=gp_weight) 
        loss_D_list.append(float(loss_D))
        
        # loss_Dã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³ã€ã¤ã¾ã‚Šä»Šå›ãªã‚‰discriminatorã¨generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¾®åˆ†å€¤ã‚’æ›¸ãæ›ãˆã¦ãã‚Œã¨ã„ã†ã“ã¨
        loss_D.backward()
        # otimizerã«ã—ãŸãŒã£ã¦ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’æ›´æ–°
        optimizer_D.step()
        
        
        # -----------------
        #  Train Generator
        # -----------------
        
        # discriminatorã‚’opt.n_criticå›å­¦ç¿’ã•ã›ã‚‹ã”ã¨ã«ä¸€å›generatorã‚’å­¦ç¿’ã•ã›ã‚‹(ãŸã ã—æœ€åˆã¯ã‚ã£ã¡ã‚ƒdiscriminatorã‚’å„ªå…ˆã•ã›ã‚‹)
        if batches_done % (100 if generator_done<25 or generator_done%500==0 else opt.n_critic) == 0:
            
            for param in discriminator.parameters():
                param.requires_grad = False # discriminatorã®å‹¾é…ã®æ›´æ–°ã‚’OFFã«ã™ã‚‹

            # generatorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–(discriminatorã®å­¦ç¿’æ™‚ã«å‹¾é…æƒ…å ±ãŒæ›´æ–°ã•ã‚Œã¦ã—ã¾ã£ã¦ã„ã‚‹ãŸã‚)
            optimizer_G.zero_grad()

            # ç¾åœ¨ï¼‹éå»pæ™‚åˆ»åˆ†ã®æ™‚ç³»åˆ—ã‚’generatorã§å¤‰æ›ã—ãŸå€¤ã‚’å–å¾—(ã‚‚ã†ä¸€å›å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã‹ãª...?)
            hat_normeps_t = generator(X)# torch.Size([64, 4])

            # Adversarial loss(discriminatorã®å‡ºåŠ›ã®æœŸå¾…å€¤ã‚’å¤§ããã—ã¦ã€ã¤ã¾ã‚ŠWassersteinè·é›¢ã®ç¬¬äºŒé …ã‚’å¤§ããã—ã¦ã€Wassersteinè·é›¢å°ã•ãã—ãŸã„)
            loss_G = -torch.mean(discriminator(hat_normeps_t))
            # ã“ã“ã«ç›¸é–¢ä¿‚æ•°ã‚’å°ã•ãã™ã‚‹ãƒ­ã‚¹ã‚‚åŠ ãˆã‚‹ï¼Ÿ
            if opt.withCorr:
                loss_G = loss_G + corr_weight*corr(hat_normeps_t)
            loss_G_list.append(float(loss_G))
            
            # loss_Gã‚’ç›®çš„é–¢æ•°ã¨ã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¾®åˆ†ã‚’ã—ã¦ãã‚Œã¨è¨€ã†åˆå›³
            loss_G.backward()
            # generatorã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãã®å¾®åˆ†å€¤ã¨optimizerã‚’ä½¿ã£ã¦æ›´æ–°ã—ã¦ãã‚Œï¼
            optimizer_G.step()
            
            generator_done+=1

        
        # -----------------
        #  Train Predictor
        # -----------------
        
        # discriminatorã‚’opt.n_criticå›å­¦ç¿’ã•ã›ã‚‹ã”ã¨ã«ä¸€å›predictorã‚’å­¦ç¿’ã•ã›ã‚‹(ãŸã ã—æœ€åˆã¯ã‚ã£ã¡ã‚ƒdiscriminatorã‚’å„ªå…ˆã•ã›ã‚‹)
        if batches_done % (100 if predictor_done<25 or predictor_done%500==0 else opt.n_critic) == 0: # ã¨ã‚Šã‚ãˆãšgeneratorã¨åŒã˜ï¼Ÿ
        
            # generatorã¨predictorã®å‹¾é…æƒ…å ±ã‚’0ã«åˆæœŸåŒ–(ä¸€å¿œã€‚å‰å›ã®æ›´æ–°æ™‚ã®å‹¾é…æƒ…å ±ã‚’ãªãã™ãŸã‚ï¼Ÿ)
            optimizer_G.zero_grad()
            optimizer_F.zero_grad()
            
            # æ­£è¦åŒ–ã•ã‚ŒãŸinnoationã®æ¨å®šé‡ã‚’generatorã‚’ç”¨ã„ã¦ç®—å‡º
            hat_normeps_t = generator(X)
            # ã“ã‚Œã¨éå»pæ™‚åˆ»ã®æ™‚ç³»åˆ—ã®å€¤ï¼ˆX_{t-1}, .... , X_{t-p}ï¼‰ã‚’predictorã¸å…¥åŠ›
            input_tensor = torch.cat([hat_normeps_t.view(opt.batch_size, -1, 1), X[:,:,:-1]], dim=2)
            prediction = predictor(input_tensor)
            
            loss_F = mseLoss(prediction, X[:,:,-1])
            loss_F_list.append(float(loss_F))
            
            
            loss_F.backward()
            optimizer_F.step()
            optimizer_G.step()
            
            predictor_done+=1
        batches_done+=1

    print("epochï¼š{0}/{1}   batchï¼š{2:003}/{3}   loss_Dï¼š{4: .4f}   loss_Gï¼š{5: .4f}   loss_Fï¼š{6: .4f}   çµŒéæ™‚é–“ï¼š{7: .1f}ç§’".format(epoch, opt.n_epochs, i+1, len(batch_sample), round(float(loss_D), 4), round(float(loss_G), 4), round(float(loss_F), 4), time.time()-start))
            
    if saveModel and epoch % opt.sample_interval == 0:
        torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/generator_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
        torch.save(discriminator.state_dict(), 'parameters/p'+str(p)+'/discriminator_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
        torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/predictor_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))

    # epochã”ã¨ã«batchã§è¨ˆç®—ã—ãŸlossã‚’å¹³å‡ã—ãŸå€¤ã‚’loss_curveã¨ã—ã¦æããŸã„
    loss_D_curve.append(sum(loss_D_list)/len(loss_D_list))
    loss_G_curve.append(sum(loss_G_list)/len(loss_G_list))
    loss_F_curve.append(sum(loss_F_list)/len(loss_F_list))
    
    # validationãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹lossã‚‚è¨ˆç®—ã—ãŸã„
    val_hat_normeps_t = generator(valMatrix)
    val_normeps_t = torch.randn_like(val_hat_normeps_t)
    val_input_tensor = torch.cat([val_hat_normeps_t.view(-1, 1,1), valMatrix[:,:,:-1]], dim=2)
    
    val_loss_D = -torch.mean(discriminator(val_normeps_t)) + torch.mean(discriminator(val_hat_normeps_t))
    if opt.withGP:
        val_loss_D = val_loss_D + gradient_penalty(generated_data=val_hat_normeps_t, real_data=val_normeps_t, gp_weight=gp_weight) 
    val_loss_D_curve.append(float(val_loss_D))
    
    val_loss_G = -torch.mean(discriminator(val_hat_normeps_t))
    if opt.withCorr:
        val_loss_G = val_loss_G + corr_weight*corr(val_hat_normeps_t)
    val_loss_G_curve.append(float(val_loss_G))
    
    val_loss_F = mseLoss(predictor(val_input_tensor), valMatrix[:,:,0])
    val_loss_F_curve.append(float(val_loss_F))
    
    if epoch % 10==0:
        plt.figure(figsize=(13,8))
        plt.title("Discriminatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_D_curve, label="training")
        plt.plot(val_loss_D_curve, label="validation")
        plt.legend()
        plt.savefig("dloss.png")
        plt.close()

        plt.figure(figsize=(13,8))
        plt.title("Generatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_G_curve, label="training")
        plt.plot(val_loss_G_curve, label="validation")
        plt.legend()
        plt.savefig("gloss.png")
        plt.close()
        
        plt.figure(figsize=(13,8))
        plt.title("Predictorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
        plt.xlabel("epoch")
        plt.ylabel("Loss")
        plt.plot(loss_F_curve, label="training")
        plt.plot(val_loss_F_curve, label="validation")
        plt.legend()
        plt.savefig("floss.png")
        plt.close()
    
torch.save(generator.state_dict(), 'parameters/p'+str(p)+'/generator_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
torch.save(discriminator.state_dict(), 'parameters/p'+str(p)+'/discriminator_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
torch.save(predictor.state_dict(), 'parameters/p'+str(p)+'/predictor_epoch{0}_batchSize{1}_GP{2}_Corr{3}_DataSeed{4}.pth'.format(epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))

plt.figure(figsize=(13,8))
plt.title("Discriminatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_D_curve, label="training")
plt.plot(val_loss_D_curve, label="validation")
plt.legend()
plt.savefig("output-images/p{0}/dloss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
# plt.show()
plt.close()

plt.figure(figsize=(13,8))
plt.title("Generatorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_G_curve, label="training")
plt.plot(val_loss_G_curve, label="validation")
plt.legend()
plt.savefig("output-images/p{0}/gloss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
# plt.show()
plt.close()

plt.figure(figsize=(13,8))
plt.title("Predictorã®Lossã®é·ç§»ã€€\nã€€batchSize:{0}, GPã®ä¿‚æ•°:{1}, Corrã®ä¿‚æ•°:{2}".format(opt.batch_size, gp_weight, corr_weight))
plt.xlabel("epoch")
plt.ylabel("Loss")
plt.plot(loss_F_curve, label="training")
plt.plot(val_loss_F_curve, label="validation")
plt.legend()
plt.savefig("output-images/p{0}/floss_epoch{1}_batchSize{2}_GP{3}_Corr{4}_DataSeed{5}.png".format(p, epoch, opt.batch_size, int(gp_weight), int(corr_weight), dataSeed ))
# plt.show()
plt.close()