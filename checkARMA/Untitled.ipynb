{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "path = os.getcwd()\n",
    "path=path[:path.find('estimate-innovation')+20]\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(path+\"/\")\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 人工データを生成してくれる機械が置いてあるところ\n",
    "import tsModel\n",
    "# 学習用のニューラルネットが置いてあるところ\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_p = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output-images/p{0}\".format(hat_p), exist_ok=True)\n",
    "os.makedirs(\"parameters/p{0}\".format(hat_p), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, data_seed=0, discriminator_hidden_unit=64, discriminator_seed=0, generator_seed=0, lr=5e-05, n_critic=5, n_epochs=20000, p=7, random_seed=0, sample_interval=1000, withCorr=False, withGP=False)\n",
      "discriminatorのパラメータをclipする値(正の数)を入力してください(defaultは0.01)：1\n",
      "clip値は 1.0 です。\n"
     ]
    }
   ],
   "source": [
    "# 学習時のハイパラの決定（入力を受け付ける）\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20000, help=\"Discriminatorを学習させる回数\")\n",
    "parser.add_argument(\"--p\", type=int, default=7, help=\"ARの次数(generatorへの入力の次元)\")\n",
    "parser.add_argument(\"--generator_seed\", type=int, default=0, help=\"generatorのパラメータの初期値のシード\")\n",
    "parser.add_argument(\"--discriminator_seed\", type=int, default=0, help=\"discriminatorのパラメータの初期値のシード\")\n",
    "parser.add_argument(\"--random_seed\", type=int, default=0, help=\"訓練データの時系列のどの時刻を学習に用いるかをランダムに決定する時のシード\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"batchの大きさ\")\n",
    "parser.add_argument(\"--discriminator_hidden_unit\", type=int, default=64, help=\"discriminatorの隠れ層のニューロンの数\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"学習率\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"一度generatorをbackpropするごとに何回discriminatorをbackpropするか\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=1000, help=\"batchを何回学習させる度にgeneratorの出力を保存するか\")\n",
    "parser.add_argument(\"--withGP\", type=bool, default=False, help=\"clipingの代わりにGradient Penaltyを加えるかどうか。True/False\")\n",
    "parser.add_argument(\"--withCorr\", type=bool, default=False, help=\"Generatorの出力がbatch方向に無相関になるようなロスを加えるかどうか。　True/False\")\n",
    "parser.add_argument(\"--data_seed\", type=int, default=0, help=\"Dataの作成に用いる乱数のseed\")\n",
    "# opt = parser.parse_args()\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "print(opt)\n",
    "\n",
    "if not opt.withGP:\n",
    "    clip_value = input('discriminatorのパラメータをclipする値(正の数)を入力してください(defaultは0.01)：')\n",
    "    try:\n",
    "        clip_value=float(clip_value)\n",
    "        if clip_value<=0:\n",
    "            clip_value=0.01\n",
    "    except:\n",
    "        clip_value=0.01\n",
    "    print(\"clip値は\",clip_value,\"です。\")\n",
    "\n",
    "# 相関係数の制約をLossに加える際の重みの設定\n",
    "default_weight = 1\n",
    "if opt.withCorr:\n",
    "    corr_weight= input(\"相関係数の制約をLossに加える時の重みを入力してください。(defaultは{0})：\".format(default_weight))\n",
    "    try:\n",
    "        corr_weight = float(corr_weight)\n",
    "        if corr_weight<0:\n",
    "            corr_weight = default_weight\n",
    "    except:\n",
    "        corr_weight=default_weight\n",
    "    print(\"相関係数に関する制約のLoss内での重みは{0}です。\".format(corr_weight))\n",
    "        \n",
    "else:\n",
    "    corr_weight=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUは使えません。\n"
     ]
    }
   ],
   "source": [
    "# gpuが使えるかどうか\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    print(\"GPUが使えます。\")\n",
    "    use_gpu = input('GPUを使いますか？ （Yes：1, No：0）  ----> ')\n",
    "    cuda = bool(int(use_gpu))\n",
    "else:\n",
    "    print(\"GPUは使えません。\")\n",
    "    \n",
    "if cuda:\n",
    "    gpu_id = input('使用するGPUの番号を入れてください : ')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
    "device = torch.device('cuda:'+gpu_id if cuda else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(opt.generator_seed)\n",
    "generator = models.LinearGenerator(p = opt.p, input_dim=1, is_bias=False)\n",
    "\n",
    "torch.manual_seed(opt.discriminator_seed)\n",
    "discriminator = models.Discriminator(q=0, discriminator_hidden_unit=opt.discriminator_hidden_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人工データの作成\n",
    "Data = tsModel.SARIMA(a=[0.3,-0.4,0.3,-0.4,0.3,-0.4,0.3], N=1000, random_seed=opt.data_seed, sigma=2)\n",
    "Data = torch.tensor(Data, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=Data.view(1,-1)\n",
    "\n",
    "trainData = Data[:,:900]\n",
    "valData = Data[:,900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMatrix = []\n",
    "for i in range(trainData.shape[1]-(opt.p+1)):\n",
    "    ans = trainData[:,i:i+opt.p+1].view(1,Data.shape[0],-1)\n",
    "    trainMatrix.append(ans)\n",
    "trainMatrix = torch.cat(trainMatrix)\n",
    "\n",
    "valMatrix = []\n",
    "for i in range(valData.shape[1]-(opt.p+1)):\n",
    "    ans = valData[:,i:i+opt.p+1].view(1,Data.shape[0],-1)\n",
    "    valMatrix.append(ans)\n",
    "valMatrix = torch.cat(valMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "\n",
    "# Optimizers(パラメータに対して定義される)\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)\n",
    "\n",
    "# パラメータと学習データをGPUに乗っける\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "trainMatrix=trainMatrix.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(generated_data, real_data, gp_weight=10):\n",
    "\n",
    "    batch_size = real_data.size()[0]\n",
    "\n",
    "    alpha = torch.rand(batch_size, 1)\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "    if cuda:\n",
    "        alpha=alpha.to(device)\n",
    "\n",
    "    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
    "    interpolated = Variable(interpolated, requires_grad=True)\n",
    "    if cuda:\n",
    "        interpolated=interpolated.to(device)\n",
    "\n",
    "    # Calculate probability of interpolated examples\n",
    "    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(prob_interpolated.size()).to(device) if cuda else torch.ones(\n",
    "                               prob_interpolated.size()),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)# これいらないかも...\n",
    "    \n",
    "    # gradients_norm = (gradients.norm(2, dim=1) - 1) ** 2\n",
    "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)# 0除算を防ぐ？\n",
    "\n",
    "    return gp_weight * ((gradients_norm - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x):\n",
    "    return (((x-x.mean())*(x-x.mean()).T)*(1-torch.eye(x.shape[0],x.shape[0]).to(device))).sum()/2/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作成したモデルを保存しますか？ （Yes：1, No：0）  ----> 0\n"
     ]
    }
   ],
   "source": [
    "saveModel = input('作成したモデルを保存しますか？ （Yes：1, No：0）  ----> ')\n",
    "saveModel = bool(int(saveModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import japanize_matplotlib\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20000] [Batch 0/13] [D loss: 0.001664] [G loss: 0.056777]\n",
      "[Epoch 8/20000] [Batch 1/13] [D loss: -0.008913] [G loss: 0.109947]\n",
      "[Epoch 16/20000] [Batch 3/13] [D loss: -0.053516] [G loss: 0.195784]\n",
      "[Epoch 24/20000] [Batch 4/13] [D loss: -0.006213] [G loss: 0.199228]\n",
      "[Epoch 31/20000] [Batch 6/13] [D loss: -0.187188] [G loss: 0.437850]\n",
      "[Epoch 39/20000] [Batch 7/13] [D loss: -0.345979] [G loss: 0.610280]\n",
      "[Epoch 47/20000] [Batch 9/13] [D loss: -0.122760] [G loss: 0.535613]\n",
      "[Epoch 54/20000] [Batch 10/13] [D loss: -0.405887] [G loss: 1.006418]\n",
      "[Epoch 62/20000] [Batch 12/13] [D loss: -0.626594] [G loss: 1.235066]\n",
      "[Epoch 70/20000] [Batch 0/13] [D loss: -0.075233] [G loss: 1.349363]\n",
      "[Epoch 77/20000] [Batch 1/13] [D loss: -0.711275] [G loss: 1.741642]\n",
      "[Epoch 85/20000] [Batch 3/13] [D loss: -0.906193] [G loss: 2.627325]\n",
      "[Epoch 93/20000] [Batch 4/13] [D loss: -1.547985] [G loss: 3.252793]\n",
      "[Epoch 101/20000] [Batch 6/13] [D loss: -1.635839] [G loss: 3.441990]\n",
      "[Epoch 108/20000] [Batch 7/13] [D loss: -0.038757] [G loss: 3.606790]\n",
      "[Epoch 116/20000] [Batch 9/13] [D loss: -2.123519] [G loss: 4.894552]\n",
      "[Epoch 124/20000] [Batch 11/13] [D loss: 0.040594] [G loss: 4.226377]\n",
      "[Epoch 131/20000] [Batch 12/13] [D loss: -3.713029] [G loss: 7.975012]\n",
      "[Epoch 139/20000] [Batch 0/13] [D loss: -4.440797] [G loss: 8.885758]\n",
      "[Epoch 147/20000] [Batch 1/13] [D loss: -3.570943] [G loss: 8.023222]\n",
      "[Epoch 154/20000] [Batch 3/13] [D loss: -2.339832] [G loss: 9.164650]\n",
      "[Epoch 162/20000] [Batch 4/13] [D loss: -5.493785] [G loss: 11.438207]\n",
      "[Epoch 170/20000] [Batch 6/13] [D loss: -3.199353] [G loss: 13.510696]\n",
      "[Epoch 177/20000] [Batch 8/13] [D loss: -8.306294] [G loss: 16.820629]\n",
      "[Epoch 185/20000] [Batch 9/13] [D loss: -8.196780] [G loss: 16.134451]\n",
      "[Epoch 186/20000] [Batch 9/13] [D loss: -6.539906] [G loss: 15.612970]\n",
      "[Epoch 186/20000] [Batch 9/13] [D loss: -10.104315] [G loss: 20.573900]\n",
      "[Epoch 186/20000] [Batch 9/13] [D loss: -7.350860] [G loss: 20.502235]\n",
      "[Epoch 187/20000] [Batch 9/13] [D loss: -6.283849] [G loss: 16.055182]\n",
      "[Epoch 187/20000] [Batch 10/13] [D loss: -8.936014] [G loss: 21.343630]\n",
      "[Epoch 187/20000] [Batch 10/13] [D loss: 0.351852] [G loss: 15.572981]\n",
      "[Epoch 188/20000] [Batch 10/13] [D loss: -6.274672] [G loss: 17.990837]\n",
      "[Epoch 188/20000] [Batch 10/13] [D loss: -8.361980] [G loss: 19.807220]\n",
      "[Epoch 189/20000] [Batch 10/13] [D loss: -2.486975] [G loss: 14.235719]\n",
      "[Epoch 189/20000] [Batch 10/13] [D loss: -7.086399] [G loss: 20.915882]\n",
      "[Epoch 189/20000] [Batch 10/13] [D loss: -10.952919] [G loss: 21.057737]\n",
      "[Epoch 190/20000] [Batch 10/13] [D loss: -7.846624] [G loss: 18.373503]\n",
      "[Epoch 190/20000] [Batch 10/13] [D loss: -3.369679] [G loss: 15.940218]\n",
      "[Epoch 191/20000] [Batch 10/13] [D loss: -9.421223] [G loss: 21.909975]\n",
      "[Epoch 191/20000] [Batch 10/13] [D loss: -11.869813] [G loss: 20.956726]\n",
      "[Epoch 191/20000] [Batch 10/13] [D loss: -11.657655] [G loss: 24.464788]\n",
      "[Epoch 192/20000] [Batch 10/13] [D loss: -11.780869] [G loss: 21.603909]\n",
      "[Epoch 192/20000] [Batch 11/13] [D loss: -9.302323] [G loss: 22.612980]\n",
      "[Epoch 192/20000] [Batch 11/13] [D loss: -10.916147] [G loss: 23.953348]\n",
      "[Epoch 193/20000] [Batch 11/13] [D loss: -13.520519] [G loss: 24.822258]\n",
      "[Epoch 193/20000] [Batch 11/13] [D loss: -9.400208] [G loss: 21.389610]\n",
      "[Epoch 194/20000] [Batch 11/13] [D loss: -3.609242] [G loss: 15.618997]\n",
      "[Epoch 194/20000] [Batch 11/13] [D loss: -8.604691] [G loss: 22.587618]\n",
      "[Epoch 194/20000] [Batch 11/13] [D loss: -3.323815] [G loss: 19.315744]\n",
      "[Epoch 195/20000] [Batch 11/13] [D loss: -6.465622] [G loss: 18.376724]\n",
      "[Epoch 195/20000] [Batch 11/13] [D loss: -11.012310] [G loss: 23.339327]\n",
      "[Epoch 196/20000] [Batch 11/13] [D loss: -8.756176] [G loss: 21.222202]\n",
      "[Epoch 196/20000] [Batch 11/13] [D loss: -6.351470] [G loss: 18.686001]\n",
      "[Epoch 196/20000] [Batch 11/13] [D loss: -11.009551] [G loss: 21.511919]\n",
      "[Epoch 197/20000] [Batch 11/13] [D loss: -7.516294] [G loss: 22.667980]\n",
      "[Epoch 197/20000] [Batch 12/13] [D loss: -9.304185] [G loss: 23.041611]\n",
      "[Epoch 197/20000] [Batch 12/13] [D loss: -4.701843] [G loss: 22.236849]\n",
      "[Epoch 198/20000] [Batch 12/13] [D loss: -9.083714] [G loss: 22.178249]\n",
      "[Epoch 198/20000] [Batch 12/13] [D loss: -0.761066] [G loss: 18.388941]\n",
      "[Epoch 199/20000] [Batch 12/13] [D loss: -0.197648] [G loss: 15.915362]\n",
      "[Epoch 199/20000] [Batch 12/13] [D loss: -9.926228] [G loss: 21.875359]\n",
      "[Epoch 199/20000] [Batch 12/13] [D loss: -5.814440] [G loss: 19.299650]\n",
      "[Epoch 200/20000] [Batch 12/13] [D loss: -5.754199] [G loss: 21.153521]\n",
      "[Epoch 200/20000] [Batch 12/13] [D loss: -6.779471] [G loss: 19.839487]\n",
      "[Epoch 201/20000] [Batch 12/13] [D loss: -11.473474] [G loss: 25.075542]\n",
      "[Epoch 201/20000] [Batch 12/13] [D loss: -8.161705] [G loss: 23.355995]\n",
      "[Epoch 201/20000] [Batch 12/13] [D loss: -11.996386] [G loss: 23.207579]\n",
      "[Epoch 202/20000] [Batch 12/13] [D loss: -6.843547] [G loss: 19.775755]\n",
      "[Epoch 202/20000] [Batch 13/13] [D loss: -11.788252] [G loss: 24.767342]\n",
      "[Epoch 202/20000] [Batch 13/13] [D loss: -7.211487] [G loss: 19.567640]\n",
      "[Epoch 203/20000] [Batch 13/13] [D loss: -9.887960] [G loss: 28.166981]\n",
      "[Epoch 203/20000] [Batch 13/13] [D loss: -8.944804] [G loss: 22.326004]\n",
      "[Epoch 204/20000] [Batch 13/13] [D loss: -7.662264] [G loss: 18.636097]\n",
      "[Epoch 204/20000] [Batch 13/13] [D loss: -10.097654] [G loss: 26.453499]\n",
      "[Epoch 204/20000] [Batch 13/13] [D loss: -10.079941] [G loss: 21.710741]\n",
      "[Epoch 205/20000] [Batch 13/13] [D loss: -10.079954] [G loss: 21.862909]\n",
      "[Epoch 205/20000] [Batch 13/13] [D loss: -9.574226] [G loss: 21.304081]\n",
      "[Epoch 206/20000] [Batch 13/13] [D loss: -11.414785] [G loss: 25.075682]\n",
      "[Epoch 206/20000] [Batch 13/13] [D loss: -8.951328] [G loss: 25.198944]\n",
      "[Epoch 206/20000] [Batch 13/13] [D loss: -5.430529] [G loss: 25.030502]\n",
      "[Epoch 207/20000] [Batch 0/13] [D loss: -10.819020] [G loss: 24.555208]\n",
      "[Epoch 207/20000] [Batch 0/13] [D loss: -11.642099] [G loss: 26.859093]\n",
      "[Epoch 207/20000] [Batch 0/13] [D loss: -4.463671] [G loss: 20.431662]\n",
      "[Epoch 208/20000] [Batch 0/13] [D loss: -8.434258] [G loss: 23.732321]\n",
      "[Epoch 208/20000] [Batch 0/13] [D loss: -7.932085] [G loss: 25.785042]\n",
      "[Epoch 209/20000] [Batch 0/13] [D loss: -5.319447] [G loss: 18.957550]\n",
      "[Epoch 209/20000] [Batch 0/13] [D loss: -5.247770] [G loss: 24.452381]\n",
      "[Epoch 209/20000] [Batch 0/13] [D loss: -8.771805] [G loss: 25.094799]\n",
      "[Epoch 210/20000] [Batch 0/13] [D loss: -7.552885] [G loss: 24.705376]\n",
      "[Epoch 210/20000] [Batch 0/13] [D loss: -2.102655] [G loss: 23.525299]\n",
      "[Epoch 211/20000] [Batch 0/13] [D loss: -10.475520] [G loss: 25.602608]\n",
      "[Epoch 211/20000] [Batch 0/13] [D loss: -12.186201] [G loss: 26.887012]\n",
      "[Epoch 211/20000] [Batch 1/13] [D loss: -11.372856] [G loss: 27.460791]\n",
      "[Epoch 212/20000] [Batch 1/13] [D loss: -7.529154] [G loss: 25.719950]\n",
      "[Epoch 212/20000] [Batch 1/13] [D loss: -6.832027] [G loss: 27.564074]\n",
      "[Epoch 212/20000] [Batch 1/13] [D loss: -11.766169] [G loss: 27.041597]\n",
      "[Epoch 213/20000] [Batch 1/13] [D loss: -16.821407] [G loss: 31.454288]\n",
      "[Epoch 213/20000] [Batch 1/13] [D loss: -8.225494] [G loss: 27.997635]\n",
      "[Epoch 214/20000] [Batch 1/13] [D loss: -2.028870] [G loss: 20.135479]\n",
      "[Epoch 214/20000] [Batch 1/13] [D loss: -11.527163] [G loss: 25.957554]\n",
      "[Epoch 214/20000] [Batch 1/13] [D loss: -12.561850] [G loss: 29.584410]\n",
      "[Epoch 215/20000] [Batch 1/13] [D loss: -1.066027] [G loss: 19.174021]\n",
      "[Epoch 215/20000] [Batch 1/13] [D loss: -6.177979] [G loss: 24.886593]\n",
      "[Epoch 216/20000] [Batch 1/13] [D loss: -13.300756] [G loss: 31.377815]\n",
      "[Epoch 216/20000] [Batch 1/13] [D loss: -5.422970] [G loss: 24.281668]\n",
      "[Epoch 216/20000] [Batch 2/13] [D loss: -11.265741] [G loss: 28.745043]\n",
      "[Epoch 217/20000] [Batch 2/13] [D loss: -4.531303] [G loss: 24.645245]\n",
      "[Epoch 217/20000] [Batch 2/13] [D loss: -17.285732] [G loss: 31.519531]\n",
      "[Epoch 217/20000] [Batch 2/13] [D loss: -10.621862] [G loss: 23.609650]\n",
      "[Epoch 218/20000] [Batch 2/13] [D loss: -15.130121] [G loss: 31.280640]\n",
      "[Epoch 218/20000] [Batch 2/13] [D loss: -10.360842] [G loss: 29.344114]\n",
      "[Epoch 219/20000] [Batch 2/13] [D loss: 1.110464] [G loss: 21.868967]\n",
      "[Epoch 219/20000] [Batch 2/13] [D loss: -10.843317] [G loss: 31.950172]\n",
      "[Epoch 219/20000] [Batch 2/13] [D loss: -16.045025] [G loss: 30.606745]\n",
      "[Epoch 220/20000] [Batch 2/13] [D loss: -11.013941] [G loss: 26.294979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 220/20000] [Batch 2/13] [D loss: -15.010731] [G loss: 34.290508]\n",
      "[Epoch 221/20000] [Batch 2/13] [D loss: -4.842201] [G loss: 27.607449]\n",
      "[Epoch 221/20000] [Batch 2/13] [D loss: -11.441437] [G loss: 28.295053]\n",
      "[Epoch 221/20000] [Batch 3/13] [D loss: -21.921776] [G loss: 37.340420]\n",
      "[Epoch 222/20000] [Batch 3/13] [D loss: -15.170586] [G loss: 32.130959]\n",
      "[Epoch 222/20000] [Batch 3/13] [D loss: -16.261457] [G loss: 32.834858]\n",
      "[Epoch 222/20000] [Batch 3/13] [D loss: -15.046759] [G loss: 35.995541]\n",
      "[Epoch 223/20000] [Batch 3/13] [D loss: -12.751072] [G loss: 33.677277]\n",
      "[Epoch 223/20000] [Batch 3/13] [D loss: -11.565760] [G loss: 27.524563]\n",
      "[Epoch 224/20000] [Batch 3/13] [D loss: -7.518471] [G loss: 23.416183]\n",
      "[Epoch 224/20000] [Batch 3/13] [D loss: -12.078306] [G loss: 33.607979]\n",
      "[Epoch 224/20000] [Batch 3/13] [D loss: -8.476768] [G loss: 29.300211]\n",
      "[Epoch 225/20000] [Batch 3/13] [D loss: -9.052065] [G loss: 28.638271]\n",
      "[Epoch 225/20000] [Batch 3/13] [D loss: -19.445784] [G loss: 36.416443]\n",
      "[Epoch 226/20000] [Batch 3/13] [D loss: -15.693027] [G loss: 35.141727]\n",
      "[Epoch 226/20000] [Batch 3/13] [D loss: -17.600523] [G loss: 32.626957]\n",
      "[Epoch 226/20000] [Batch 4/13] [D loss: -14.254602] [G loss: 37.237354]\n",
      "[Epoch 227/20000] [Batch 4/13] [D loss: -14.695736] [G loss: 33.608871]\n",
      "[Epoch 227/20000] [Batch 4/13] [D loss: -19.302027] [G loss: 35.977650]\n",
      "[Epoch 227/20000] [Batch 4/13] [D loss: -4.349195] [G loss: 25.927238]\n",
      "[Epoch 228/20000] [Batch 4/13] [D loss: -19.323767] [G loss: 37.430157]\n",
      "[Epoch 228/20000] [Batch 4/13] [D loss: -16.604195] [G loss: 32.294914]\n",
      "[Epoch 229/20000] [Batch 4/13] [D loss: -5.623863] [G loss: 24.393314]\n",
      "[Epoch 229/20000] [Batch 4/13] [D loss: -22.129068] [G loss: 35.700958]\n",
      "[Epoch 229/20000] [Batch 4/13] [D loss: -16.767225] [G loss: 34.440315]\n",
      "[Epoch 230/20000] [Batch 4/13] [D loss: -12.599157] [G loss: 31.220783]\n",
      "[Epoch 230/20000] [Batch 4/13] [D loss: -4.583017] [G loss: 29.200283]\n",
      "[Epoch 231/20000] [Batch 4/13] [D loss: -17.832296] [G loss: 36.876671]\n",
      "[Epoch 231/20000] [Batch 4/13] [D loss: -12.548279] [G loss: 34.550388]\n",
      "[Epoch 231/20000] [Batch 5/13] [D loss: -15.203869] [G loss: 35.222168]\n",
      "[Epoch 232/20000] [Batch 5/13] [D loss: -13.215363] [G loss: 35.500694]\n",
      "[Epoch 232/20000] [Batch 5/13] [D loss: -16.262997] [G loss: 31.048506]\n",
      "[Epoch 232/20000] [Batch 5/13] [D loss: -12.246517] [G loss: 33.571724]\n",
      "[Epoch 233/20000] [Batch 5/13] [D loss: -14.501163] [G loss: 32.771908]\n",
      "[Epoch 233/20000] [Batch 5/13] [D loss: -13.588333] [G loss: 30.994829]\n",
      "[Epoch 234/20000] [Batch 5/13] [D loss: -4.178619] [G loss: 26.960022]\n",
      "[Epoch 234/20000] [Batch 5/13] [D loss: -15.199150] [G loss: 35.801472]\n",
      "[Epoch 234/20000] [Batch 5/13] [D loss: -20.792189] [G loss: 39.099125]\n",
      "[Epoch 235/20000] [Batch 5/13] [D loss: -11.395491] [G loss: 33.340977]\n",
      "[Epoch 235/20000] [Batch 5/13] [D loss: -1.881781] [G loss: 30.472359]\n",
      "[Epoch 236/20000] [Batch 5/13] [D loss: -14.111570] [G loss: 36.406864]\n",
      "[Epoch 236/20000] [Batch 6/13] [D loss: -21.986412] [G loss: 38.645329]\n",
      "[Epoch 236/20000] [Batch 6/13] [D loss: -12.454887] [G loss: 40.129417]\n",
      "[Epoch 237/20000] [Batch 6/13] [D loss: -14.193527] [G loss: 37.936226]\n",
      "[Epoch 237/20000] [Batch 6/13] [D loss: -20.365068] [G loss: 39.053566]\n",
      "[Epoch 237/20000] [Batch 6/13] [D loss: -8.085289] [G loss: 30.368706]\n",
      "[Epoch 238/20000] [Batch 6/13] [D loss: -23.190170] [G loss: 43.738075]\n",
      "[Epoch 238/20000] [Batch 6/13] [D loss: -16.194412] [G loss: 35.618340]\n",
      "[Epoch 239/20000] [Batch 6/13] [D loss: -5.988579] [G loss: 28.076288]\n",
      "[Epoch 239/20000] [Batch 6/13] [D loss: -11.202339] [G loss: 35.546616]\n",
      "[Epoch 239/20000] [Batch 6/13] [D loss: -8.702541] [G loss: 34.526970]\n",
      "[Epoch 240/20000] [Batch 6/13] [D loss: -7.898335] [G loss: 34.604355]\n",
      "[Epoch 240/20000] [Batch 6/13] [D loss: -11.965363] [G loss: 33.022530]\n",
      "[Epoch 241/20000] [Batch 6/13] [D loss: -6.264339] [G loss: 29.386353]\n",
      "[Epoch 241/20000] [Batch 7/13] [D loss: -8.788479] [G loss: 37.680737]\n",
      "[Epoch 241/20000] [Batch 7/13] [D loss: -22.312521] [G loss: 44.117889]\n",
      "[Epoch 242/20000] [Batch 7/13] [D loss: -7.774891] [G loss: 40.401596]\n",
      "[Epoch 242/20000] [Batch 7/13] [D loss: -22.721363] [G loss: 41.534115]\n",
      "[Epoch 242/20000] [Batch 7/13] [D loss: -20.577290] [G loss: 47.921249]\n",
      "[Epoch 243/20000] [Batch 7/13] [D loss: -14.317169] [G loss: 38.332603]\n",
      "[Epoch 243/20000] [Batch 7/13] [D loss: -23.790438] [G loss: 42.183491]\n",
      "[Epoch 244/20000] [Batch 7/13] [D loss: -9.361496] [G loss: 32.118149]\n",
      "[Epoch 244/20000] [Batch 7/13] [D loss: -16.056463] [G loss: 39.765434]\n",
      "[Epoch 244/20000] [Batch 7/13] [D loss: -20.073011] [G loss: 41.777843]\n",
      "[Epoch 245/20000] [Batch 7/13] [D loss: -14.679832] [G loss: 38.887329]\n",
      "[Epoch 245/20000] [Batch 7/13] [D loss: -16.766951] [G loss: 43.203300]\n",
      "[Epoch 246/20000] [Batch 7/13] [D loss: -8.997841] [G loss: 39.473362]\n",
      "[Epoch 246/20000] [Batch 8/13] [D loss: -16.622782] [G loss: 40.896259]\n",
      "[Epoch 246/20000] [Batch 8/13] [D loss: -10.587269] [G loss: 39.217957]\n",
      "[Epoch 247/20000] [Batch 8/13] [D loss: -15.714754] [G loss: 44.905975]\n",
      "[Epoch 247/20000] [Batch 8/13] [D loss: -10.530174] [G loss: 43.246826]\n",
      "[Epoch 247/20000] [Batch 8/13] [D loss: -11.087296] [G loss: 41.708778]\n",
      "[Epoch 248/20000] [Batch 8/13] [D loss: -18.525185] [G loss: 41.295578]\n",
      "[Epoch 248/20000] [Batch 8/13] [D loss: -23.402946] [G loss: 35.847515]\n",
      "[Epoch 249/20000] [Batch 8/13] [D loss: -6.692245] [G loss: 33.092693]\n",
      "[Epoch 249/20000] [Batch 8/13] [D loss: -27.684500] [G loss: 47.892525]\n",
      "[Epoch 249/20000] [Batch 8/13] [D loss: -20.698338] [G loss: 42.832035]\n",
      "[Epoch 250/20000] [Batch 8/13] [D loss: -5.249313] [G loss: 37.899139]\n",
      "[Epoch 250/20000] [Batch 8/13] [D loss: -11.465202] [G loss: 36.889893]\n",
      "[Epoch 251/20000] [Batch 8/13] [D loss: -22.199221] [G loss: 48.127060]\n",
      "[Epoch 251/20000] [Batch 9/13] [D loss: -15.363688] [G loss: 42.350677]\n",
      "[Epoch 251/20000] [Batch 9/13] [D loss: -21.980343] [G loss: 52.542912]\n",
      "[Epoch 252/20000] [Batch 9/13] [D loss: -13.114662] [G loss: 37.554569]\n",
      "[Epoch 252/20000] [Batch 9/13] [D loss: -20.330200] [G loss: 44.449188]\n",
      "[Epoch 252/20000] [Batch 9/13] [D loss: -16.995235] [G loss: 44.612617]\n",
      "[Epoch 253/20000] [Batch 9/13] [D loss: -19.622755] [G loss: 47.557976]\n",
      "[Epoch 253/20000] [Batch 9/13] [D loss: -14.158201] [G loss: 45.039700]\n",
      "[Epoch 254/20000] [Batch 9/13] [D loss: -5.334538] [G loss: 33.976727]\n",
      "[Epoch 254/20000] [Batch 9/13] [D loss: -29.427105] [G loss: 51.803867]\n",
      "[Epoch 254/20000] [Batch 9/13] [D loss: -13.990063] [G loss: 38.057350]\n",
      "[Epoch 255/20000] [Batch 9/13] [D loss: -10.268927] [G loss: 40.755436]\n",
      "[Epoch 255/20000] [Batch 9/13] [D loss: -30.238142] [G loss: 50.553890]\n",
      "[Epoch 256/20000] [Batch 9/13] [D loss: -11.516232] [G loss: 37.114693]\n",
      "[Epoch 256/20000] [Batch 10/13] [D loss: -14.015209] [G loss: 46.349998]\n",
      "[Epoch 256/20000] [Batch 10/13] [D loss: -32.818668] [G loss: 58.612789]\n",
      "[Epoch 257/20000] [Batch 10/13] [D loss: -12.706034] [G loss: 39.746655]\n",
      "[Epoch 257/20000] [Batch 10/13] [D loss: -18.069839] [G loss: 51.345795]\n",
      "[Epoch 257/20000] [Batch 10/13] [D loss: -31.029860] [G loss: 57.010273]\n",
      "[Epoch 258/20000] [Batch 10/13] [D loss: -27.115513] [G loss: 49.508339]\n",
      "[Epoch 258/20000] [Batch 10/13] [D loss: -30.043114] [G loss: 49.781219]\n",
      "[Epoch 259/20000] [Batch 10/13] [D loss: -12.451908] [G loss: 37.456898]\n",
      "[Epoch 259/20000] [Batch 10/13] [D loss: -12.128605] [G loss: 45.867290]\n",
      "[Epoch 259/20000] [Batch 10/13] [D loss: -21.321743] [G loss: 52.475559]\n",
      "[Epoch 260/20000] [Batch 10/13] [D loss: -15.861864] [G loss: 40.891682]\n",
      "[Epoch 260/20000] [Batch 10/13] [D loss: -14.649424] [G loss: 41.095688]\n",
      "[Epoch 261/20000] [Batch 11/13] [D loss: -4.071980] [G loss: 40.330853]\n",
      "[Epoch 261/20000] [Batch 11/13] [D loss: -23.558016] [G loss: 52.491776]\n",
      "[Epoch 261/20000] [Batch 11/13] [D loss: -26.228767] [G loss: 53.753441]\n",
      "[Epoch 262/20000] [Batch 11/13] [D loss: -20.812000] [G loss: 53.614098]\n",
      "[Epoch 262/20000] [Batch 11/13] [D loss: -31.914434] [G loss: 54.837116]\n",
      "[Epoch 262/20000] [Batch 11/13] [D loss: -29.192495] [G loss: 57.712917]\n",
      "[Epoch 263/20000] [Batch 11/13] [D loss: -20.931133] [G loss: 53.100529]\n",
      "[Epoch 263/20000] [Batch 11/13] [D loss: -16.952396] [G loss: 49.142529]\n",
      "[Epoch 264/20000] [Batch 11/13] [D loss: -1.200293] [G loss: 32.979488]\n",
      "[Epoch 264/20000] [Batch 11/13] [D loss: -27.835968] [G loss: 53.178009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 264/20000] [Batch 11/13] [D loss: -13.892679] [G loss: 42.939636]\n",
      "[Epoch 265/20000] [Batch 11/13] [D loss: -15.233644] [G loss: 41.331856]\n",
      "[Epoch 265/20000] [Batch 11/13] [D loss: -27.170322] [G loss: 55.665024]\n",
      "[Epoch 266/20000] [Batch 12/13] [D loss: -28.437338] [G loss: 59.097984]\n",
      "[Epoch 266/20000] [Batch 12/13] [D loss: -16.445805] [G loss: 50.377060]\n",
      "[Epoch 266/20000] [Batch 12/13] [D loss: -37.098007] [G loss: 64.192627]\n",
      "[Epoch 267/20000] [Batch 12/13] [D loss: -12.568611] [G loss: 44.675522]\n",
      "[Epoch 267/20000] [Batch 12/13] [D loss: -33.690018] [G loss: 52.862816]\n",
      "[Epoch 267/20000] [Batch 12/13] [D loss: -13.040207] [G loss: 44.409218]\n",
      "[Epoch 268/20000] [Batch 12/13] [D loss: -10.380997] [G loss: 48.249142]\n",
      "[Epoch 268/20000] [Batch 12/13] [D loss: -18.495747] [G loss: 54.176632]\n",
      "[Epoch 269/20000] [Batch 12/13] [D loss: -3.312099] [G loss: 37.205269]\n",
      "[Epoch 269/20000] [Batch 12/13] [D loss: -20.183365] [G loss: 59.747917]\n",
      "[Epoch 269/20000] [Batch 12/13] [D loss: -18.672188] [G loss: 56.768616]\n",
      "[Epoch 270/20000] [Batch 12/13] [D loss: -14.333002] [G loss: 37.952602]\n",
      "[Epoch 270/20000] [Batch 12/13] [D loss: -14.212620] [G loss: 46.364849]\n",
      "[Epoch 271/20000] [Batch 13/13] [D loss: -26.382036] [G loss: 53.302086]\n",
      "[Epoch 271/20000] [Batch 13/13] [D loss: -34.761745] [G loss: 61.442360]\n",
      "[Epoch 271/20000] [Batch 13/13] [D loss: -29.748272] [G loss: 61.951790]\n",
      "[Epoch 272/20000] [Batch 13/13] [D loss: -11.673847] [G loss: 52.956474]\n",
      "[Epoch 272/20000] [Batch 13/13] [D loss: -18.667038] [G loss: 50.604088]\n",
      "[Epoch 272/20000] [Batch 13/13] [D loss: -5.395683] [G loss: 45.937714]\n",
      "[Epoch 273/20000] [Batch 13/13] [D loss: -25.482693] [G loss: 63.739319]\n",
      "[Epoch 273/20000] [Batch 13/13] [D loss: -15.095734] [G loss: 56.383583]\n",
      "[Epoch 274/20000] [Batch 13/13] [D loss: -6.102314] [G loss: 43.250000]\n",
      "[Epoch 274/20000] [Batch 13/13] [D loss: -18.807320] [G loss: 51.849754]\n",
      "[Epoch 274/20000] [Batch 13/13] [D loss: -27.981367] [G loss: 54.282059]\n",
      "[Epoch 275/20000] [Batch 13/13] [D loss: 5.981853] [G loss: 38.668560]\n",
      "[Epoch 275/20000] [Batch 0/13] [D loss: -18.681652] [G loss: 47.538696]\n",
      "[Epoch 276/20000] [Batch 0/13] [D loss: -21.367573] [G loss: 57.300907]\n",
      "[Epoch 276/20000] [Batch 0/13] [D loss: -15.153172] [G loss: 49.109207]\n",
      "[Epoch 276/20000] [Batch 0/13] [D loss: -46.158226] [G loss: 72.531425]\n",
      "[Epoch 277/20000] [Batch 0/13] [D loss: -12.424538] [G loss: 47.550877]\n",
      "[Epoch 277/20000] [Batch 0/13] [D loss: -27.878059] [G loss: 66.392143]\n",
      "[Epoch 277/20000] [Batch 0/13] [D loss: -15.058445] [G loss: 46.662922]\n",
      "[Epoch 278/20000] [Batch 0/13] [D loss: -41.776814] [G loss: 67.707191]\n",
      "[Epoch 278/20000] [Batch 0/13] [D loss: -36.705017] [G loss: 67.430260]\n",
      "[Epoch 279/20000] [Batch 0/13] [D loss: -11.661854] [G loss: 45.307175]\n",
      "[Epoch 279/20000] [Batch 0/13] [D loss: -36.040142] [G loss: 58.118748]\n",
      "[Epoch 279/20000] [Batch 0/13] [D loss: -5.594151] [G loss: 48.162708]\n",
      "[Epoch 280/20000] [Batch 0/13] [D loss: -11.503197] [G loss: 40.701065]\n",
      "[Epoch 280/20000] [Batch 1/13] [D loss: -14.374241] [G loss: 50.910744]\n",
      "[Epoch 281/20000] [Batch 1/13] [D loss: -16.802790] [G loss: 48.607647]\n",
      "[Epoch 281/20000] [Batch 1/13] [D loss: -19.684471] [G loss: 58.207840]\n",
      "[Epoch 281/20000] [Batch 1/13] [D loss: -22.133427] [G loss: 60.707840]\n",
      "[Epoch 282/20000] [Batch 1/13] [D loss: -24.191334] [G loss: 51.852734]\n",
      "[Epoch 282/20000] [Batch 1/13] [D loss: -14.561035] [G loss: 63.818031]\n",
      "[Epoch 282/20000] [Batch 1/13] [D loss: -33.147228] [G loss: 71.324356]\n",
      "[Epoch 283/20000] [Batch 1/13] [D loss: -29.700954] [G loss: 59.193787]\n",
      "[Epoch 283/20000] [Batch 1/13] [D loss: -29.948643] [G loss: 58.650135]\n",
      "[Epoch 284/20000] [Batch 1/13] [D loss: -10.547367] [G loss: 45.703659]\n",
      "[Epoch 284/20000] [Batch 1/13] [D loss: -27.880466] [G loss: 60.593605]\n",
      "[Epoch 284/20000] [Batch 1/13] [D loss: -15.713459] [G loss: 55.219822]\n",
      "[Epoch 285/20000] [Batch 1/13] [D loss: -13.767147] [G loss: 47.657166]\n",
      "[Epoch 285/20000] [Batch 2/13] [D loss: -41.941742] [G loss: 71.193634]\n",
      "[Epoch 286/20000] [Batch 2/13] [D loss: -21.785381] [G loss: 52.630867]\n",
      "[Epoch 286/20000] [Batch 2/13] [D loss: -24.840023] [G loss: 71.132614]\n",
      "[Epoch 286/20000] [Batch 2/13] [D loss: -11.824753] [G loss: 68.080902]\n",
      "[Epoch 287/20000] [Batch 2/13] [D loss: -17.608795] [G loss: 52.301506]\n",
      "[Epoch 287/20000] [Batch 2/13] [D loss: -20.515499] [G loss: 60.304066]\n",
      "[Epoch 287/20000] [Batch 2/13] [D loss: -8.515678] [G loss: 51.492653]\n",
      "[Epoch 288/20000] [Batch 2/13] [D loss: -25.651924] [G loss: 58.552853]\n",
      "[Epoch 288/20000] [Batch 2/13] [D loss: -20.616627] [G loss: 63.271732]\n",
      "[Epoch 289/20000] [Batch 2/13] [D loss: -20.153601] [G loss: 51.445492]\n",
      "[Epoch 289/20000] [Batch 2/13] [D loss: -35.560036] [G loss: 68.520950]\n",
      "[Epoch 289/20000] [Batch 2/13] [D loss: -40.822784] [G loss: 67.083557]\n",
      "[Epoch 290/20000] [Batch 3/13] [D loss: -15.925613] [G loss: 53.900902]\n",
      "[Epoch 290/20000] [Batch 3/13] [D loss: -23.337105] [G loss: 54.272556]\n",
      "[Epoch 291/20000] [Batch 3/13] [D loss: -18.675369] [G loss: 60.145336]\n",
      "[Epoch 291/20000] [Batch 3/13] [D loss: -19.971741] [G loss: 62.902729]\n",
      "[Epoch 291/20000] [Batch 3/13] [D loss: -27.378304] [G loss: 73.522064]\n",
      "[Epoch 292/20000] [Batch 3/13] [D loss: -23.809834] [G loss: 65.982330]\n",
      "[Epoch 292/20000] [Batch 3/13] [D loss: -28.811131] [G loss: 65.961197]\n",
      "[Epoch 292/20000] [Batch 3/13] [D loss: -11.555622] [G loss: 54.584450]\n",
      "[Epoch 293/20000] [Batch 3/13] [D loss: -34.256256] [G loss: 75.509583]\n",
      "[Epoch 293/20000] [Batch 3/13] [D loss: -14.769829] [G loss: 46.646713]\n",
      "[Epoch 294/20000] [Batch 3/13] [D loss: -24.532921] [G loss: 53.419209]\n",
      "[Epoch 294/20000] [Batch 3/13] [D loss: -18.071739] [G loss: 71.793289]\n",
      "[Epoch 294/20000] [Batch 3/13] [D loss: -29.294788] [G loss: 73.626099]\n",
      "[Epoch 295/20000] [Batch 4/13] [D loss: -21.440254] [G loss: 64.413422]\n",
      "[Epoch 295/20000] [Batch 4/13] [D loss: -14.675129] [G loss: 59.317291]\n",
      "[Epoch 296/20000] [Batch 4/13] [D loss: -9.628792] [G loss: 55.833286]\n",
      "[Epoch 296/20000] [Batch 4/13] [D loss: -28.001720] [G loss: 73.721138]\n",
      "[Epoch 296/20000] [Batch 4/13] [D loss: -33.735138] [G loss: 69.398331]\n",
      "[Epoch 297/20000] [Batch 4/13] [D loss: -34.476517] [G loss: 73.088577]\n",
      "[Epoch 297/20000] [Batch 4/13] [D loss: -25.544453] [G loss: 66.126358]\n",
      "[Epoch 297/20000] [Batch 4/13] [D loss: -23.090103] [G loss: 62.847469]\n",
      "[Epoch 298/20000] [Batch 4/13] [D loss: -32.463737] [G loss: 73.116386]\n",
      "[Epoch 298/20000] [Batch 4/13] [D loss: -0.956314] [G loss: 48.436031]\n",
      "[Epoch 299/20000] [Batch 4/13] [D loss: -24.765440] [G loss: 51.232201]\n",
      "[Epoch 299/20000] [Batch 4/13] [D loss: -28.961746] [G loss: 70.903099]\n",
      "[Epoch 299/20000] [Batch 4/13] [D loss: -15.260712] [G loss: 60.981602]\n",
      "[Epoch 300/20000] [Batch 5/13] [D loss: -17.717995] [G loss: 63.822445]\n",
      "[Epoch 300/20000] [Batch 5/13] [D loss: -45.527634] [G loss: 82.703720]\n",
      "[Epoch 301/20000] [Batch 5/13] [D loss: -18.197639] [G loss: 60.257061]\n",
      "[Epoch 301/20000] [Batch 5/13] [D loss: -16.858246] [G loss: 62.912754]\n",
      "[Epoch 301/20000] [Batch 5/13] [D loss: -51.717518] [G loss: 83.246758]\n",
      "[Epoch 302/20000] [Batch 5/13] [D loss: -18.605034] [G loss: 75.585846]\n",
      "[Epoch 302/20000] [Batch 5/13] [D loss: -36.685001] [G loss: 75.849319]\n",
      "[Epoch 302/20000] [Batch 5/13] [D loss: -35.878708] [G loss: 84.475021]\n",
      "[Epoch 303/20000] [Batch 5/13] [D loss: -39.526844] [G loss: 85.267220]\n",
      "[Epoch 303/20000] [Batch 5/13] [D loss: -33.106354] [G loss: 75.120911]\n",
      "[Epoch 304/20000] [Batch 5/13] [D loss: -5.562138] [G loss: 57.890034]\n",
      "[Epoch 304/20000] [Batch 5/13] [D loss: -19.473690] [G loss: 68.904388]\n",
      "[Epoch 304/20000] [Batch 5/13] [D loss: -30.906639] [G loss: 80.598312]\n",
      "[Epoch 305/20000] [Batch 6/13] [D loss: -29.295059] [G loss: 69.819038]\n",
      "[Epoch 305/20000] [Batch 6/13] [D loss: -50.254345] [G loss: 85.246109]\n",
      "[Epoch 306/20000] [Batch 6/13] [D loss: 0.394333] [G loss: 59.318291]\n",
      "[Epoch 306/20000] [Batch 6/13] [D loss: -30.212502] [G loss: 75.071098]\n",
      "[Epoch 306/20000] [Batch 6/13] [D loss: -37.928379] [G loss: 86.098625]\n",
      "[Epoch 307/20000] [Batch 6/13] [D loss: -28.166348] [G loss: 67.861259]\n",
      "[Epoch 307/20000] [Batch 6/13] [D loss: -30.148476] [G loss: 74.135460]\n",
      "[Epoch 307/20000] [Batch 6/13] [D loss: -46.158997] [G loss: 90.066444]\n",
      "[Epoch 308/20000] [Batch 6/13] [D loss: -25.710453] [G loss: 73.195633]\n",
      "[Epoch 308/20000] [Batch 6/13] [D loss: -32.012520] [G loss: 63.283981]\n",
      "[Epoch 309/20000] [Batch 6/13] [D loss: -8.338692] [G loss: 49.471012]\n",
      "[Epoch 309/20000] [Batch 6/13] [D loss: -37.701706] [G loss: 87.485260]\n",
      "[Epoch 309/20000] [Batch 6/13] [D loss: -15.126198] [G loss: 73.148048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 310/20000] [Batch 7/13] [D loss: -27.571449] [G loss: 65.853943]\n",
      "[Epoch 310/20000] [Batch 7/13] [D loss: -14.077133] [G loss: 66.831360]\n",
      "[Epoch 311/20000] [Batch 7/13] [D loss: -29.147430] [G loss: 64.494637]\n",
      "[Epoch 311/20000] [Batch 7/13] [D loss: -34.142895] [G loss: 78.431259]\n",
      "[Epoch 311/20000] [Batch 7/13] [D loss: -46.333279] [G loss: 79.931870]\n",
      "[Epoch 312/20000] [Batch 7/13] [D loss: -18.584000] [G loss: 79.970245]\n",
      "[Epoch 312/20000] [Batch 7/13] [D loss: -32.047485] [G loss: 76.856308]\n",
      "[Epoch 312/20000] [Batch 7/13] [D loss: -11.171410] [G loss: 64.968704]\n",
      "[Epoch 313/20000] [Batch 7/13] [D loss: -22.137802] [G loss: 74.256676]\n",
      "[Epoch 313/20000] [Batch 7/13] [D loss: -34.689983] [G loss: 77.568314]\n",
      "[Epoch 314/20000] [Batch 7/13] [D loss: -7.424507] [G loss: 62.838966]\n",
      "[Epoch 314/20000] [Batch 7/13] [D loss: -45.396961] [G loss: 87.291611]\n",
      "[Epoch 314/20000] [Batch 8/13] [D loss: -39.486732] [G loss: 86.806541]\n",
      "[Epoch 315/20000] [Batch 8/13] [D loss: -26.874302] [G loss: 69.398819]\n",
      "[Epoch 315/20000] [Batch 8/13] [D loss: -7.615246] [G loss: 68.090027]\n",
      "[Epoch 316/20000] [Batch 8/13] [D loss: -29.196949] [G loss: 64.751663]\n",
      "[Epoch 316/20000] [Batch 8/13] [D loss: -43.604015] [G loss: 91.436363]\n",
      "[Epoch 316/20000] [Batch 8/13] [D loss: -39.898182] [G loss: 85.466522]\n",
      "[Epoch 317/20000] [Batch 8/13] [D loss: -18.793449] [G loss: 88.356857]\n",
      "[Epoch 317/20000] [Batch 8/13] [D loss: -40.583988] [G loss: 80.359375]\n",
      "[Epoch 317/20000] [Batch 8/13] [D loss: -27.399891] [G loss: 82.611183]\n",
      "[Epoch 318/20000] [Batch 8/13] [D loss: -30.187996] [G loss: 82.986061]\n",
      "[Epoch 318/20000] [Batch 8/13] [D loss: -43.254692] [G loss: 88.263626]\n",
      "[Epoch 319/20000] [Batch 8/13] [D loss: -4.231331] [G loss: 67.479614]\n",
      "[Epoch 319/20000] [Batch 8/13] [D loss: -34.708801] [G loss: 85.168716]\n",
      "[Epoch 319/20000] [Batch 9/13] [D loss: -23.072124] [G loss: 72.933098]\n",
      "[Epoch 320/20000] [Batch 9/13] [D loss: -23.929424] [G loss: 67.739471]\n",
      "[Epoch 320/20000] [Batch 9/13] [D loss: -9.437370] [G loss: 73.537918]\n",
      "[Epoch 321/20000] [Batch 9/13] [D loss: -22.993389] [G loss: 70.975792]\n",
      "[Epoch 321/20000] [Batch 9/13] [D loss: -37.648239] [G loss: 84.335983]\n",
      "[Epoch 321/20000] [Batch 9/13] [D loss: -64.506119] [G loss: 110.406693]\n",
      "[Epoch 322/20000] [Batch 9/13] [D loss: -13.004005] [G loss: 72.984718]\n",
      "[Epoch 322/20000] [Batch 9/13] [D loss: -38.038399] [G loss: 101.277794]\n",
      "[Epoch 322/20000] [Batch 9/13] [D loss: -21.562088] [G loss: 85.422386]\n",
      "[Epoch 323/20000] [Batch 9/13] [D loss: -33.190834] [G loss: 80.894257]\n",
      "[Epoch 323/20000] [Batch 9/13] [D loss: -31.806877] [G loss: 73.536377]\n",
      "[Epoch 324/20000] [Batch 9/13] [D loss: -26.762814] [G loss: 71.069740]\n",
      "[Epoch 324/20000] [Batch 9/13] [D loss: -21.586704] [G loss: 85.593369]\n",
      "[Epoch 324/20000] [Batch 10/13] [D loss: -49.445999] [G loss: 91.836769]\n",
      "[Epoch 325/20000] [Batch 10/13] [D loss: -32.777775] [G loss: 84.649117]\n",
      "[Epoch 325/20000] [Batch 10/13] [D loss: -41.335960] [G loss: 103.802711]\n",
      "[Epoch 326/20000] [Batch 10/13] [D loss: -64.955055] [G loss: 104.241333]\n",
      "[Epoch 326/20000] [Batch 10/13] [D loss: -50.620796] [G loss: 87.158173]\n",
      "[Epoch 326/20000] [Batch 10/13] [D loss: -39.076828] [G loss: 90.149017]\n",
      "[Epoch 327/20000] [Batch 10/13] [D loss: -34.573009] [G loss: 88.714439]\n",
      "[Epoch 327/20000] [Batch 10/13] [D loss: -52.318142] [G loss: 101.318054]\n",
      "[Epoch 327/20000] [Batch 10/13] [D loss: -31.327240] [G loss: 78.311737]\n",
      "[Epoch 328/20000] [Batch 10/13] [D loss: -20.514420] [G loss: 84.460449]\n",
      "[Epoch 328/20000] [Batch 10/13] [D loss: -3.684258] [G loss: 64.204117]\n",
      "[Epoch 329/20000] [Batch 10/13] [D loss: -26.529701] [G loss: 71.507172]\n",
      "[Epoch 329/20000] [Batch 10/13] [D loss: -26.968910] [G loss: 91.853798]\n",
      "[Epoch 329/20000] [Batch 11/13] [D loss: -30.375927] [G loss: 83.405998]\n",
      "[Epoch 330/20000] [Batch 11/13] [D loss: -37.676895] [G loss: 87.838921]\n",
      "[Epoch 330/20000] [Batch 11/13] [D loss: -48.639961] [G loss: 106.854286]\n",
      "[Epoch 331/20000] [Batch 11/13] [D loss: -23.357319] [G loss: 79.432999]\n",
      "[Epoch 331/20000] [Batch 11/13] [D loss: -47.327015] [G loss: 94.193756]\n",
      "[Epoch 331/20000] [Batch 11/13] [D loss: -47.080441] [G loss: 102.761322]\n",
      "[Epoch 332/20000] [Batch 11/13] [D loss: -53.897064] [G loss: 102.429138]\n",
      "[Epoch 332/20000] [Batch 11/13] [D loss: -50.768520] [G loss: 106.140610]\n",
      "[Epoch 332/20000] [Batch 11/13] [D loss: -53.213505] [G loss: 112.559402]\n",
      "[Epoch 333/20000] [Batch 11/13] [D loss: -56.452972] [G loss: 112.457809]\n",
      "[Epoch 333/20000] [Batch 11/13] [D loss: -55.651917] [G loss: 102.329567]\n",
      "[Epoch 334/20000] [Batch 11/13] [D loss: -32.296947] [G loss: 71.098099]\n",
      "[Epoch 334/20000] [Batch 11/13] [D loss: -55.518944] [G loss: 103.660606]\n",
      "[Epoch 334/20000] [Batch 12/13] [D loss: -51.456528] [G loss: 100.795319]\n",
      "[Epoch 335/20000] [Batch 12/13] [D loss: -32.020580] [G loss: 86.121590]\n",
      "[Epoch 335/20000] [Batch 12/13] [D loss: -27.903164] [G loss: 74.179451]\n",
      "[Epoch 336/20000] [Batch 12/13] [D loss: -35.265884] [G loss: 93.244202]\n",
      "[Epoch 336/20000] [Batch 12/13] [D loss: -43.485825] [G loss: 110.097565]\n",
      "[Epoch 336/20000] [Batch 12/13] [D loss: -52.054131] [G loss: 110.818474]\n",
      "[Epoch 337/20000] [Batch 12/13] [D loss: -23.136318] [G loss: 81.217415]\n",
      "[Epoch 337/20000] [Batch 12/13] [D loss: -51.105850] [G loss: 110.581703]\n",
      "[Epoch 337/20000] [Batch 12/13] [D loss: -40.590080] [G loss: 98.786835]\n",
      "[Epoch 338/20000] [Batch 12/13] [D loss: -60.623959] [G loss: 117.273727]\n",
      "[Epoch 338/20000] [Batch 12/13] [D loss: -55.750854] [G loss: 98.803711]\n",
      "[Epoch 339/20000] [Batch 12/13] [D loss: -4.154152] [G loss: 69.802269]\n",
      "[Epoch 339/20000] [Batch 13/13] [D loss: -58.912445] [G loss: 108.293442]\n",
      "[Epoch 339/20000] [Batch 13/13] [D loss: -32.640186] [G loss: 91.174309]\n",
      "[Epoch 340/20000] [Batch 13/13] [D loss: -19.537628] [G loss: 72.789062]\n",
      "[Epoch 340/20000] [Batch 13/13] [D loss: -12.230629] [G loss: 88.692520]\n",
      "[Epoch 341/20000] [Batch 13/13] [D loss: -34.491768] [G loss: 81.239868]\n",
      "[Epoch 341/20000] [Batch 13/13] [D loss: -59.469582] [G loss: 111.222076]\n",
      "[Epoch 341/20000] [Batch 13/13] [D loss: -29.417107] [G loss: 104.350403]\n",
      "[Epoch 342/20000] [Batch 13/13] [D loss: -16.514145] [G loss: 86.871292]\n",
      "[Epoch 342/20000] [Batch 13/13] [D loss: -48.459961] [G loss: 107.622131]\n",
      "[Epoch 342/20000] [Batch 13/13] [D loss: -45.217472] [G loss: 103.084000]\n",
      "[Epoch 343/20000] [Batch 13/13] [D loss: -65.595863] [G loss: 123.503960]\n",
      "[Epoch 343/20000] [Batch 13/13] [D loss: -37.858444] [G loss: 109.414917]\n",
      "[Epoch 344/20000] [Batch 0/13] [D loss: -17.445290] [G loss: 82.370346]\n",
      "[Epoch 344/20000] [Batch 0/13] [D loss: -70.287201] [G loss: 120.637985]\n",
      "[Epoch 344/20000] [Batch 0/13] [D loss: -39.516441] [G loss: 98.655029]\n",
      "[Epoch 345/20000] [Batch 0/13] [D loss: -5.590721] [G loss: 72.225899]\n",
      "[Epoch 345/20000] [Batch 0/13] [D loss: -71.168892] [G loss: 124.971771]\n",
      "[Epoch 346/20000] [Batch 0/13] [D loss: -37.152847] [G loss: 121.176880]\n",
      "[Epoch 346/20000] [Batch 0/13] [D loss: -43.556618] [G loss: 108.745560]\n",
      "[Epoch 346/20000] [Batch 0/13] [D loss: -32.325378] [G loss: 105.018639]\n",
      "[Epoch 347/20000] [Batch 0/13] [D loss: -45.051193] [G loss: 112.584076]\n",
      "[Epoch 347/20000] [Batch 0/13] [D loss: -39.705444] [G loss: 116.176270]\n",
      "[Epoch 347/20000] [Batch 0/13] [D loss: -65.461105] [G loss: 128.498459]\n",
      "[Epoch 348/20000] [Batch 0/13] [D loss: -57.139969] [G loss: 118.897888]\n",
      "[Epoch 348/20000] [Batch 0/13] [D loss: -44.245491] [G loss: 107.335205]\n",
      "[Epoch 349/20000] [Batch 1/13] [D loss: -5.182930] [G loss: 87.965355]\n",
      "[Epoch 349/20000] [Batch 1/13] [D loss: -46.362457] [G loss: 115.624222]\n",
      "[Epoch 349/20000] [Batch 1/13] [D loss: -58.172726] [G loss: 121.133026]\n",
      "[Epoch 350/20000] [Batch 1/13] [D loss: -9.629089] [G loss: 78.456573]\n",
      "[Epoch 350/20000] [Batch 1/13] [D loss: -44.629452] [G loss: 99.323334]\n",
      "[Epoch 351/20000] [Batch 1/13] [D loss: -18.586220] [G loss: 91.859566]\n",
      "[Epoch 351/20000] [Batch 1/13] [D loss: -43.501350] [G loss: 112.805939]\n",
      "[Epoch 351/20000] [Batch 1/13] [D loss: -45.825607] [G loss: 111.941414]\n",
      "[Epoch 352/20000] [Batch 1/13] [D loss: -36.060303] [G loss: 104.533615]\n",
      "[Epoch 352/20000] [Batch 1/13] [D loss: -50.389786] [G loss: 114.741547]\n",
      "[Epoch 352/20000] [Batch 1/13] [D loss: -27.980782] [G loss: 111.470154]\n",
      "[Epoch 353/20000] [Batch 1/13] [D loss: -57.235306] [G loss: 109.926674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 353/20000] [Batch 1/13] [D loss: -49.829819] [G loss: 116.529808]\n",
      "[Epoch 354/20000] [Batch 2/13] [D loss: -14.955910] [G loss: 91.617325]\n",
      "[Epoch 354/20000] [Batch 2/13] [D loss: -38.046272] [G loss: 114.685104]\n",
      "[Epoch 354/20000] [Batch 2/13] [D loss: -40.780273] [G loss: 108.550354]\n",
      "[Epoch 355/20000] [Batch 2/13] [D loss: -26.474174] [G loss: 86.640564]\n",
      "[Epoch 355/20000] [Batch 2/13] [D loss: -62.434494] [G loss: 129.229218]\n",
      "[Epoch 356/20000] [Batch 2/13] [D loss: -57.598068] [G loss: 119.351952]\n",
      "[Epoch 356/20000] [Batch 2/13] [D loss: -49.803680] [G loss: 115.244186]\n",
      "[Epoch 356/20000] [Batch 2/13] [D loss: -34.244370] [G loss: 120.312164]\n",
      "[Epoch 357/20000] [Batch 2/13] [D loss: -31.367905] [G loss: 96.110062]\n",
      "[Epoch 357/20000] [Batch 2/13] [D loss: -61.859879] [G loss: 123.208733]\n",
      "[Epoch 357/20000] [Batch 2/13] [D loss: -52.967545] [G loss: 131.690506]\n",
      "[Epoch 358/20000] [Batch 2/13] [D loss: -64.776665] [G loss: 137.029999]\n",
      "[Epoch 358/20000] [Batch 2/13] [D loss: -55.953880] [G loss: 128.057373]\n",
      "[Epoch 359/20000] [Batch 3/13] [D loss: -20.437378] [G loss: 92.208694]\n",
      "[Epoch 359/20000] [Batch 3/13] [D loss: -71.583885] [G loss: 138.884720]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-14145c2e9431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsillon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_from_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhat_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_from_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhat_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsillon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsillon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_from_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhat_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_from_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/test/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/estimate-innovation/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, is_from_generator)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/test/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/test/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/test/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batches_done = 0\n",
    "generator_done = 0# generatorを学習した回数\n",
    "\n",
    "# グラフ描画用\n",
    "loss_D_curve = []\n",
    "loss_G_curve = []\n",
    "val_loss_D_curve = []\n",
    "val_loss_G_curve = []\n",
    "p_value = []\n",
    "\n",
    "# 訓練データの時系列のどの時刻を学習に用いるかをランダムにしているが、そのランダムシードを固定する\n",
    "random.seed(a=opt.random_seed)\n",
    "\n",
    "for epoch in range(1, opt.n_epochs+1):# epochごとの処理\n",
    "    \n",
    "    # trainMatrixの行をランダムにシャッフルする\n",
    "    # r=torch.randperm(trainMatrix.shape[0])\n",
    "    # c=torch.arange(trainMatrix.shape[1])\n",
    "    # trainMatrix = trainMatrix[r[:, None],c]\n",
    "    \n",
    "    for i, batch in enumerate(range(0, trainMatrix.shape[0]-opt.batch_size, opt.batch_size)):# batchごとの処理\n",
    "        \n",
    "        # generatorへの入力を用意する\n",
    "        X = trainMatrix[batch:batch+opt.batch_size]# torch.Size([64, 1, 8])\n",
    "        # 時系列の順番はそのまま入力した方がいいのかな？\n",
    "        rand=random.randint(0,trainMatrix.shape[0] - trainMatrix.shape[0]// opt.batch_size*opt.batch_size)\n",
    "        X = trainMatrix[batch+rand : batch+rand+opt.batch_size]# torch.Size([64, 1, 8])\n",
    "    \n",
    "        X = Variable(X)# 自動微分？\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        for p in discriminator.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "\n",
    "        if not opt.withGP:\n",
    "            # discriminatorのパラメタをクリップする（全てのパラメタの絶対値がclip_value以下の値になる）\n",
    "            for idx, p in enumerate(discriminator.parameters()):\n",
    "                if idx==0:\n",
    "                    continue #  sigmaはクリップしない\n",
    "                p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        \n",
    "        # 勾配情報を0に初期化する\n",
    "        optimizer_D.zero_grad()        \n",
    "        \n",
    "        # 現在＋過去p時刻分の時系列をgeneratorで変換した値を取得\n",
    "        hat_epsilon = generator(X)#.detach() # torch.Size([64, 4])\n",
    "        # この「.detach()」はTensor型から勾配情報を抜いたものを取得する.(つまりこの後の誤差逆伝播のところではgeneratorのパラメタまで伝播しない)\n",
    "        \n",
    "        # generatorの出力と同じ大きさの標準正規分布からのサンプルを作成\n",
    "        epsillon = Variable(torch.randn_like(hat_epsilon))\n",
    "        \n",
    "        # Adversarial loss すなわちWasserstein距離の符号を反転させたもの。（DiscriminatorはWasserstein距離を最大にする関数になりたい）\n",
    "        if opt.withGP:\n",
    "            loss_D = -torch.mean(discriminator(epsillon, is_from_generator=False)) + torch.mean(discriminator(hat_epsilon, is_from_generator=True)) + gradient_penalty(generated_data=hat_epsilon, real_data=epsillon, gp_weight=10)\n",
    "        else:\n",
    "            loss_D = -torch.mean(discriminator(epsillon, is_from_generator=False)) + torch.mean(discriminator(hat_epsilon, is_from_generator=True))\n",
    " \n",
    "\n",
    "        # loss_Dを目的関数として微分をしてくれと言う合図\n",
    "        loss_D.backward()\n",
    "        # otimizerにしたがってパラメタを更新\n",
    "        optimizer_D.step()\n",
    "\n",
    "            \n",
    "        # discriminatorをopt.n_critic回学習させるごとに一回generatorを学習させる(ただし最初はめっちゃdiscriminatorを優先させる)\n",
    "        if batches_done % (100 if generator_done<25 or generator_done%500==0 else opt.n_critic) == 0:\n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            \n",
    "            for p in discriminator.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            \n",
    "            # generatorの勾配情報を0に初期化\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # 現在＋過去p時刻分の時系列をgeneratorで変換した値を取得\n",
    "            hat_epsilon = generator(X)# torch.Size([64, 4]) (今度はdetachしない)\n",
    "            \n",
    "            # Adversarial loss(discriminatorの出力の期待値を大きくして、つまりWasserstein距離の第二項を大きくして、Wasserstein距離小さくしたい)\n",
    "            # ここに相関係数を小さくするロスも加える？\n",
    "            if opt.withCorr:\n",
    "                loss_G = -torch.mean(discriminator(hat_epsilon, is_from_generator=True)) + corr_weight*corr(hat_epsilon)\n",
    "            else:\n",
    "                loss_G = -torch.mean(discriminator(hat_epsilon, is_from_generator=True))\n",
    "            \n",
    "            # loss_Gを目的関数として微分をしてくれと言う合図\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            generator_done+=1\n",
    "\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, opt.n_epochs, batches_done % trainMatrix.shape[0]//opt.batch_size, trainMatrix.shape[0]//opt.batch_size, loss_D.item(), loss_G.item()) )\n",
    "            \n",
    "        \n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            # もしここでhat_epsilon保存するなら保存する\n",
    "            # hat_epsilon[:,0].shape\n",
    "            # a=hat_epsilon\n",
    "            # a=a.cpu().detach().numpy()\n",
    "            # plt.hist(a[:,0])\n",
    "            # plt.close()\n",
    "            pass\n",
    "        \n",
    "        batches_done += 1\n",
    "\n",
    "    loss_D_curve.append(loss_D.item())\n",
    "    loss_G_curve.append(loss_G.item())\n",
    "\n",
    "    #　validtation lossも付け加えたい\n",
    "    val_hat_eps = generator(valMatrix)\n",
    "    val_eps = torch.randn_like(val_hat_eps)\n",
    "    if opt.withGP:\n",
    "        val_loss_D = -torch.mean(discriminator(val_eps, is_from_generator=False)) + torch.mean(discriminator(val_hat_eps, is_from_generator=True)) + gradient_penalty(generated_data=val_hat_eps, real_data=val_eps, gp_weight=10)\n",
    "    else:\n",
    "        val_loss_D = -torch.mean(discriminator(val_eps, is_from_generator=False)) + torch.mean(discriminator(val_hat_eps, is_from_generator=True))\n",
    "    val_loss_D_curve.append(val_loss_D.item())\n",
    "    if opt.withCorr:\n",
    "        val_loss_G  = -torch.mean(discriminator(val_hat_eps, is_from_generator=True)) + corr_weight*corr(val_hat_eps)\n",
    "    else:\n",
    "        val_loss_G = -torch.mean(discriminator(val_hat_eps, is_from_generator=True))\n",
    "    val_loss_G_curve.append(val_loss_G.item())\n",
    "    \n",
    "    # validationデータでgeneratorの出力の正規性検定のp-値と相互相関係数を確認する\n",
    "    rnd = random.randint(0, valMatrix.shape[0]-64)\n",
    "    valX = valMatrix[rnd:rnd+64]\n",
    "    val_hat_epsilon = generator(valX)\n",
    "    # p-値\n",
    "    a=val_hat_epsilon[:,0].cpu().detach().numpy()\n",
    "    p_value.append(stats.shapiro(a)[1])\n",
    "\n",
    "\n",
    "    if saveModel:\n",
    "        if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:\n",
    "            torch.save(generator.state_dict(), 'parameters/generator_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'\n",
    "                       .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "            torch.save(discriminator.state_dict(), 'parameters/discriminator_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'\n",
    "                       .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "\n",
    "        if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:\n",
    "            kde = gaussian_kde(a)\n",
    "            ls = np.linspace(min(a)-np.var(a), max(a)+np.var(a) , 100)\n",
    "            plt.figure(figsize=(13,8))\n",
    "            plt.title(\"generatorの出力の分布　\\n　epoch:{0}/{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withGP:{8}, withCorr:{9}\"\n",
    "                      .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr))+\"\\n p-値：\"+str(stats.shapiro(a)[1]))\n",
    "            plt.ylabel(\"密度\")\n",
    "            plt.plot(ls, kde.pdf(ls) , label=\"$\\hat\\epsilon$\")\n",
    "            plt.plot(ls, norm.pdf(ls), label=\"$\\mathcal{N}(0,1)$\")\n",
    "            plt.legend()\n",
    "            plt.savefig(\"output-images/density_epoch{0}_{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.png\"\n",
    "                        .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "            plt.close()\n",
    "\n",
    "    if epoch%opt.sample_interval==0 or epoch==opt.n_epochs:\n",
    "        plt.figure(figsize=(13,8))\n",
    "        plt.title(\"DiscriminatorのLossの遷移　\\n　epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withGP:{8}, withCorr:{9}\"\n",
    "                  .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(loss_D_curve, label=\"training\")\n",
    "        plt.plot(val_loss_D_curve, label=\"validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"loss.png\")\n",
    "        plt.close()\n",
    "\n",
    "torch.save(generator.state_dict(), 'parameters/generator_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'\n",
    "           .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "torch.save(discriminator.state_dict(), 'parameters/discriminator_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}-withGP{8}_withCorr{9}.pth'\n",
    "           .format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の保存\n",
    "# p-値\n",
    "plt.figure(figsize=(13,8))\n",
    "plt.plot(p_value)\n",
    "plt.title(\"p-値の遷移　\\n　epoch:{1}, batchSize:{2}, randomSeed:{3}, p:{4}, gSeed:{5}, dSeed:{6}, dHiddenUnit:{7}, withPG:{8}, withCorr:{9}\".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed, opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"p-値\")\n",
    "plt.savefig(\"output-images/p-value_epoch{1}_batchSize{2}_randomSeed{3}_p{4}_gSeed{5}_dSeed{6}_dHiddenUnit{7}_withGP{8}_withCorr{9}.png\".format(epoch, opt.n_epochs, opt.batch_size, opt.random_seed, opt.p, opt.generator_seed, opt.discriminator_seed,opt.discriminator_hidden_unit, str(opt.withGP), str(opt.withCorr)))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
